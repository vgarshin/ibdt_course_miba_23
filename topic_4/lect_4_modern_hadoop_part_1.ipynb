{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09040cef",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Modern Technologies course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6671704",
   "metadata": {},
   "source": [
    "## TOPIC 4: Modern Hadoop\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7333a",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9607d1",
   "metadata": {},
   "source": [
    "### 2. Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b1824",
   "metadata": {},
   "source": [
    "#### 2.1. Data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af371bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ~/__DATA/IBDT_Spring_2023/topic_4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc465251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -R /jovyan/raw\n",
    "!hdfs dfs -rm -R /jovyan/users\n",
    "!hdfs dfs -rm -R /jovyan/instances\n",
    "!hdfs dfs -rm -R /jovyan/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2153fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /jovyan/raw\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_4/jhub_logs.csv /jovyan/raw/\n",
    "!hdfs dfs -ls /jovyan/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72e51a",
   "metadata": {},
   "source": [
    "#### 2.2. Read data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e055fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf().set('spark.master', 'local[*]') #stand-alone mode\n",
    "#conf = SparkConf().set('spark.master', 'yarn') # YARN managed\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(\n",
    "    f'/jovyan/raw/jhub_logs.csv',\n",
    "    sep=';', \n",
    "    header=True,\n",
    "    multiLine=True, # if you have `\\n` symbols\n",
    "    escape=\"\\\"\"\n",
    ")\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354ff8c",
   "metadata": {},
   "source": [
    "These two methods to be called with caution, because of calculation time and costs in a case of Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce11278",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e5c25",
   "metadata": {},
   "source": [
    "#### 3.1. Kubernetes logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('kubernetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ef1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# bad practice actually, use `limit()`\n",
    "sdf.limit(5).select('kubernetes').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0810b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong way, only for demo\n",
    "sdf.limit(5).select('log').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ac932",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# again - this is bad, use `limit()`\n",
    "sdf.limit(5).select('kubernetes').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info(rin):\n",
    "    \"\"\"\n",
    "    Extracts names of:\n",
    "      - docker image\n",
    "      - id of the Jupyter application\n",
    "      - name of the host, where Jupyter runs\n",
    "    \n",
    "    \"\"\"\n",
    "    img = rin[rin.find('container_image='):].split('\\'')[1]\n",
    "    hub = rin[rin.find('pod_name='):].split('\\'')[1]\n",
    "    host = rin[rin.find('host='):].split('\\'')[1]\n",
    "    return img, hub, host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# it is slow without `limit()`...\n",
    "row = sdf.limit(5).select('kubernetes').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2965d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = list(row)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c07fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340bdfc",
   "metadata": {},
   "source": [
    "#### 3.2. JupyterHub logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).select('log').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_brackets(sin):\n",
    "    \"\"\"\n",
    "    Split log string amd extracts:\n",
    "      - timestamp of the event\n",
    "      - name of application\n",
    "      - type of logs\n",
    "      - code of event\n",
    "      - description\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = sin.split('[', 1)[1].split(']')[0]\n",
    "        msg = sin[len(s) + 2 :].strip()\n",
    "        s = s.split()\n",
    "        head = s[0]\n",
    "        ts = ' '.join(s[1:3])\n",
    "        svc = s[3]\n",
    "        typ = s[4].split(':')[0]\n",
    "        code = s[4].split(':')[1]\n",
    "    except:\n",
    "        head, ts, svc, typ, code = '', '', '', '', ''\n",
    "        msg = sin\n",
    "    return head, ts, svc, typ, code, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51014b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = sdf.limit(5).select('log').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = list(row)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_brackets(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571942e",
   "metadata": {},
   "source": [
    "#### 3.3. Apply function Spark style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee5c23",
   "metadata": {},
   "source": [
    "Here we need Spark's user-defined functions or [udf](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26caa6f3",
   "metadata": {},
   "source": [
    "##### Kubernetes column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_row_info = udf(row_info, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98796cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('kubernetes_msg', udf_row_info('kubernetes'))\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.limit(5).select('kubernetes_msg').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc947bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    'date',\n",
    "    F.col('kubernetes_msg')[0].alias('img'),\n",
    "    F.col('kubernetes_msg')[1].alias('hub'),\n",
    "    F.col('kubernetes_msg')[2].alias('host'),\n",
    "    'log',\n",
    "    'stream',\n",
    "    'time'\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef684a91",
   "metadata": {},
   "source": [
    "__NOTE__ the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd5a9c",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c96786",
   "metadata": {},
   "source": [
    "##### Log column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04869d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_sq_brackets = udf(sq_brackets, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('log_msg', udf_sq_brackets('log'))\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    'img',\n",
    "    'hub',\n",
    "    'host',\n",
    "    F.col('log_msg')[0].alias('head'),\n",
    "    F.col('log_msg')[1].alias('timestamp'),\n",
    "    F.col('log_msg')[2].alias('service'),\n",
    "    F.col('log_msg')[3].alias('event_type'),\n",
    "    F.col('log_msg')[4].alias('event_code'),\n",
    "    F.col('log_msg')[5].alias('message')\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7229a9",
   "metadata": {},
   "source": [
    "#### 3.4. Find users' activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parce_users_activities(code, msg):\n",
    "    \"\"\"\n",
    "    Ugly function.\n",
    "    \n",
    "    You may use dictionary to make it\n",
    "    more pythonic or something else.\n",
    "    \n",
    "    \"\"\"\n",
    "    if code == '43':\n",
    "        user = msg.split()[-1]\n",
    "        log = 'logged out'\n",
    "    elif code == '757':\n",
    "        user = msg.split()[-1]\n",
    "        log = 'logged in'\n",
    "    elif code == '402':\n",
    "        user = msg.split()[0]\n",
    "        log = 'pending spawn'\n",
    "    elif code == '1875':\n",
    "        user = msg.split()[4].replace('claim-', '').replace(',', '')\n",
    "        log = 'attempt to create pvc with timeout'\n",
    "    elif code == '1887':\n",
    "        user = msg.split()[1].replace('claim-', '')\n",
    "        log = 'pvc already exists'\n",
    "    elif code == '1840':\n",
    "        user = msg.split()[4].replace('jupyter-', '').replace(',', '')\n",
    "        log = 'attempting to create pod with timeout'\n",
    "    elif code == '1344':\n",
    "        user = msg.split('/')[3]\n",
    "        log = 'failing suspected api request to not-running server'\n",
    "    elif code == '380':\n",
    "        user = msg.split()[3]\n",
    "        log = 'previous spawn failed'\n",
    "    elif code == '567':\n",
    "        user = msg.split('/')[4]\n",
    "        log = 'stream closed while handling '\n",
    "    elif code == '681':\n",
    "        user = msg.split()[0].replace('\\'s', '')\n",
    "        log = 'server failed to start'\n",
    "    elif code == '1997':\n",
    "        user = msg.split('-')[-1]\n",
    "        log = 'deleting pod'\n",
    "    elif code == '689':\n",
    "        user = msg.split()[3].replace('\\'s', '')\n",
    "        log = 'unhandled error starting with timeout'\n",
    "    elif code == '1961' or code == '2044':\n",
    "        user = msg.split()[1].replace('jupyter-', '')\n",
    "        log = 'restarting pod reflector'\n",
    "    elif code == '257':\n",
    "        user = msg.split()[2]\n",
    "        log = 'adding user to proxy'\n",
    "    elif code == '664':\n",
    "        user = msg.split()[1]\n",
    "        log = 'server is ready'\n",
    "    elif code == '61' or code == '85':\n",
    "        user = msg.split()[3]\n",
    "        log = 'spawning sever with advanced configuration option'\n",
    "    elif code == '1143':\n",
    "        user = msg.split()[1].replace(':', '')\n",
    "        log = 'server is slow to stop'\n",
    "    elif code == '2077':\n",
    "        user = msg.split()[0]\n",
    "        log = 'still running'\n",
    "    elif code == '167':\n",
    "        user = msg.split()[1]\n",
    "        log = 'server is already active'\n",
    "    elif code == '1067' or code == '2022':\n",
    "        user = msg.split()[1]\n",
    "        log = 'user server stopped with exit code 1'\n",
    "    elif code == '1857':\n",
    "        user = msg.split()[3].replace('jupyter-', '').replace(',', '')\n",
    "        log = 'found existing pod and attempting to kill'\n",
    "    elif code == '1861':\n",
    "        user = msg.split()[2].replace('jupyter-', '').replace(',', '')\n",
    "        log = 'killed pod and will try starting singleuser pod again'\n",
    "    elif code == '738':\n",
    "        user = msg.split()[0].replace(',', '').replace('\\'s', '')\n",
    "        log = 'server never showed up and giving up'\n",
    "    elif code == '2069':\n",
    "        user = msg.split()[0].replace(',', '')\n",
    "        log = 'user does not appear to be running and shutting it down'  \n",
    "    elif code == '148':\n",
    "        user = msg.split()[-1]\n",
    "        log = 'user is running'\n",
    "    elif code == '1415':\n",
    "        user = msg.split()[-1]\n",
    "        log = 'admin requesting spawn on behalf'\n",
    "    elif code == '1437':\n",
    "        user = msg.split()[5].replace(',', '')\n",
    "        log = 'user requested server which user do not own'\n",
    "    elif code == '626':\n",
    "        user = msg.split()[1]\n",
    "        log = 'server is already started'\n",
    "    elif code == '2085':\n",
    "        user = msg.split()[0]\n",
    "        log = 'server appears to have stopped while the hub was down'\n",
    "    else:\n",
    "        user, log = '', ''\n",
    "    return user, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_parce_users_activities = udf(parce_users_activities, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('user_act', udf_parce_users_activities(sdf['event_code'], sdf['message']))\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2728d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    F.col('timestamp').alias('jh_timestamp'),\n",
    "    F.col('hub').alias('jh_hub'),\n",
    "    F.col('img').alias('jh_img'),\n",
    "    F.col('host').alias('jh_host'),\n",
    "    F.col('event_code').alias('jh_event_code'),\n",
    "    F.col('event_type').alias('jh_event_type'),\n",
    "    F.col('user_act')[1].alias('jh_log'),\n",
    "    F.col('user_act')[0].alias('jh_user')\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b565bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.jh_user != '')\n",
    "sdf = sdf.withColumn(\n",
    "    'jh_timestamp',\n",
    "    F.to_timestamp(\"jh_timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c68db2",
   "metadata": {},
   "source": [
    "### 4. Normalize data (Spark way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1bff0",
   "metadata": {},
   "source": [
    "#### 4.1. Users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "logins = sdf.select('jh_user').distinct().collect()\n",
    "print(len(logins))\n",
    "logins = [list(x)[0] for x in logins]\n",
    "logins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6dec4d",
   "metadata": {},
   "source": [
    "Use `names` library https://pypi.org/project/names/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a134551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "for login in logins:\n",
    "    user = {}\n",
    "    user['login'] = login\n",
    "    user['name'] = names.get_full_name()\n",
    "    user['email'] = login + '@gsom.spbu.ru'\n",
    "    users.append(user)\n",
    "users[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([users])\n",
    "sdf_users = spark.read.json(rdd)\n",
    "sdf_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc266ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -R /jovyan/users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_users.coalesce(1).write.csv('/jovyan/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a529592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8777d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan/users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "file_name=$(hdfs dfs -ls -C /jovyan/users | grep csv)\n",
    "\n",
    "hdfs dfs -head ${file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abe48b",
   "metadata": {},
   "source": [
    "#### 4.2. JupyterHub instances table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f236480",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances = sdf.select(\n",
    "    'jh_hub',\n",
    "    'jh_img',\n",
    "    'jh_host'\n",
    ")\n",
    "sdf_instances.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d78cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances = sdf_instances.dropDuplicates()\n",
    "sdf_instances.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances.coalesce(1).write.csv('/jovyan/instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -C /jovyan/instances | grep csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "file_name=$(hdfs dfs -ls -C /jovyan/instances | grep csv)\n",
    "\n",
    "hdfs dfs -head ${file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42cd7e",
   "metadata": {},
   "source": [
    "#### 4.3. JupyterHub events table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e0ad2",
   "metadata": {},
   "source": [
    "### <font color='red'>HOME ASSIGNMENT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code will be here\n",
    "# create a table `events` with columns `event_code`, `event_type`, `log`\n",
    "# drop duplicates and save it to CSV file (to HDFS) to import to database later "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39891f97",
   "metadata": {},
   "source": [
    "#### 4.4. JupyterHub logs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79128be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that it is an example\n",
    "# you will need to keep only `event_code` column as a key\n",
    "# and remove `event_type` and `log` columns\n",
    "# for data normalization\n",
    "\n",
    "sdf_logs = sdf.select(\n",
    "    'jh_timestamp',\n",
    "    'jh_hub',\n",
    "    'jh_event_code',\n",
    "    'jh_event_type',   # to be removed\n",
    "    'jh_log',          # to be removed\n",
    "    'jh_user'\n",
    ")\n",
    "sdf_logs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_logs.coalesce(1).write.option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss.SS\").csv('/jovyan/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "file_name=$(hdfs dfs -ls -C /jovyan/logs | grep csv)\n",
    "\n",
    "hdfs dfs -head ${file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6eded",
   "metadata": {},
   "source": [
    "### 5. ETL pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc828e4",
   "metadata": {},
   "source": [
    "ETL will be developed for Hive database in the Hadoop environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f3621",
   "metadata": {},
   "source": [
    "### 6. Home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37138b10",
   "metadata": {},
   "source": [
    "Your home assignment for this part is:\n",
    "\n",
    "0. Use `Spark` instead of `Pandas` to process data\n",
    "1. Take large file with data on logs `~/__DATA/IBDT_Spring_2023/topic_1/jhub_logs_large.csv` and put in into HDFS\n",
    "2. Create a table (via `Spark` dataframe) for events (see `4.3. JupyterHub events table`) and save it as `csv` like we did with `users` and `instances` tables (HDFS!)\n",
    "3. Check your script for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af514773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
