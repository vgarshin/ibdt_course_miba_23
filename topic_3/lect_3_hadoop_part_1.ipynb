{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Modern Technologies course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC 3: Hadoop and MapReduce practice\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need port only for Web UI\n",
    "YARN_PORT = 8088\n",
    "\n",
    "# working directory for default user `jovyan`\n",
    "# `/jovyan/home` for the Jupyter \n",
    "# and `/jovyan` for the Hadoop environment\n",
    "WORK_DIR = '/jovyan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HDFS commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help is all you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigation through HDFS is available with `hdfs dfs` [commands](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html) which are quite simular to Unix shell navigation (`ls`, `cat`, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list root directory\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list directory\n",
    "!hdfs dfs -ls /jovyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or with `WORK_DIR` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list working directory '/jovyan'\n",
    "# NOTE: variable WORK_DIR='/jovyan' used in braces\n",
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Put and get files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put an arbitary file to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ~/__DATA/IBDT_Spring_2023/topic_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put local file to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/test_hdfs.txt {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the file's content\n",
    "!hdfs dfs -cat {WORK_DIR}/test_hdfs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folders and move files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir {WORK_DIR}/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv {WORK_DIR}/test_hdfs.txt {WORK_DIR}/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat {WORK_DIR}/texts/test_hdfs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get files back from `HDFS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get {WORK_DIR}/texts/test_hdfs.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Something useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_dirs(path, filter_str=''):\n",
    "    \"\"\"\n",
    "    Returns files in path provided as a list. \n",
    "    File names may be filtered by `filter_str` parameter,\n",
    "    e.g. `filter_str='csv'` will display only `csv` files.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-ls', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    dirs = out.decode('utf-8').split('\\n')\n",
    "    dirs = list(filter(lambda x: filter_str in x, dirs))\n",
    "    dirs = list(map(lambda x: x.split(' ')[-1], dirs))\n",
    "    return dirs\n",
    "\n",
    "def file_content(path):\n",
    "    \"\"\"\n",
    "    Returns content of the file.\n",
    "    Similar to `cat` command.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-cat', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    return out.decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function defined above\n",
    "hdfs_dirs(WORK_DIR, 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_dirs(WORK_DIR + '/texts', 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_dirs(WORK_DIR + '/texts', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the content of the 'telecom_churn.csv' file\n",
    "content = file_content(f'{WORK_DIR}/texts/test_hdfs.txt')\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MapReduce intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. WordCount with Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordCount` is a simple application that counts the number of occurrences of each word in a given input set. For this demo ready `jar` package is used.\n",
    "\n",
    "First let's copy files to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# create input directory on HDFS\n",
    "hdfs dfs -mkdir -p ${work_dir}/input\n",
    "\n",
    "# put files to HDFS\n",
    "hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/big_data_* ${work_dir}/input\n",
    "hdfs dfs -ls ${work_dir}/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a map-reduce job and enjoy long logs output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# delete directory if exists\n",
    "#hdfs dfs -rm -r ${work_dir}/output\n",
    "\n",
    "# run wordcount\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar wordcount \\\n",
    "    ${work_dir}/input ${work_dir}/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat {WORK_DIR}/output/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# print the output of wordcount\n",
    "echo -e \"\\nwordcount output:\"\n",
    "hdfs dfs -cat ${work_dir}/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. WordCount with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next example will use [Hadoop streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Python scripts are used `mapper.py` and `reducer.py`, let's look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo -e \"\\n************** MAPPER.PY ****************\\n\"\n",
    "cat ./utils/mapper.py\n",
    "echo -e \"\\n************** REDUCER.PY ****************\\n\"\n",
    "cat ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How this Python code works out of Hadoop\n",
    "\n",
    "First of all, a few words about bash `stdin` and `stdout`. Here is a [good article](https://medium.com/linuxstories/bash-pipes-and-redirections-4c267c13643b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat test_hdfs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# let's send our file to `stdin` of our mapper\n",
    "# `cat` is to list content of the file\n",
    "# pipe `|` is for sending that output to our `mapper.py` as input\n",
    "\n",
    "cat test_hdfs.txt | python ./utils/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# write result of mapper to the file\n",
    "\n",
    "cat test_hdfs.txt | python ./utils/mapper.py > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat result.txt | sort -t 1 | python ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat test_hdfs.txt | python ./utils/mapper.py | sort -t 1 | python ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python code within Hadoop (YARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our Python MapReduce scripts in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the job and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_py\n",
    "\n",
    "# delete directory if exists\n",
    "hdfs dfs -rm -r ${work_dir}${out_dir}\n",
    "\n",
    "yarn jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.2.jar \\\n",
    "    -input ${work_dir}/input/*.txt -output ${work_dir}${out_dir} \\\n",
    "    -file ./utils/mapper.py -file ./utils/reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" -reducer \"python3 reducer.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for Hadoop streaming:\n",
    "\n",
    "| Option | Description| \n",
    "| --- | --- |\n",
    "| -files| A command-separated list of files to be copied to the MapReduce cluster |\n",
    "| -mapper | The command to be run as the mapper |\n",
    "| -reducer | The command to be run as the reducer |\n",
    "| -input | The DFS input path for the Map step |\n",
    "| -output | The DFS output directory for the Reduce step |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_py\n",
    "\n",
    "hdfs dfs -ls ${work_dir}/${out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_py\n",
    "\n",
    "hdfs dfs -cat ${work_dir}/${out_dir}/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. YARN jobs monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop also provided YARN Web UI for Yarn Resource manager. All the jobs (submitted, running or finished) can be traced in YARN Web UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'YARN Web UI available at:',\n",
    "    'https://jhas01.gsom.spbu.ru{}proxy/{}/cluster'.format(\n",
    "        os.environ['JUPYTERHUB_SERVICE_PREFIX'],\n",
    "        YARN_PORT\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. More MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Not only word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will count the number of reviews for each rating (1, 2, 3, 4, 5) in the [Kaggle Hotels Reviews dataset](https://www.kaggle.com/datasets/yash10kundu/hotel-reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# test our scripts\n",
    "\n",
    "cat ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv | \\\n",
    "    python ./utils/mapper_rr.py | \\\n",
    "    sort -t 1 | \\\n",
    "    python ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir {WORK_DIR}/input_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now put local file to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv {WORK_DIR}/input_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now put MORE local file to HDFS (cause we can do it!)\n",
    "# we can add many files and that is how Hadoop works\n",
    "# because we can have thousands of CSV files all across many servers\n",
    "\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv {WORK_DIR}/input_csv/Hotel_Reviews_more.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/input_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_csv\n",
    "\n",
    "# delete directory if exists\n",
    "hdfs dfs -rm -r ${work_dir}${out_dir}\n",
    "\n",
    "yarn jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.2.jar \\\n",
    "    -input ${work_dir}/input_csv/*.csv -output ${work_dir}${out_dir} \\\n",
    "    -file ./utils/mapper_rr.py -file ./utils/reducer.py \\\n",
    "    -mapper \"python3 mapper_rr.py\" -reducer \"python3 reducer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_csv\n",
    "\n",
    "hdfs dfs -ls ${work_dir}/${out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_csv\n",
    "\n",
    "hdfs dfs -cat ${work_dir}/${out_dir}/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Meet MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module [mrjob](https://mrjob.readthedocs.io/en/latest/) helps to write MapReduce jobs in Python 2.7/3.4+ and run them on many platforms:\n",
    "- Write multi-step MapReduce jobs in pure Python\n",
    "- Test on your local machine\n",
    "- Run on a Hadoop cluster\n",
    "- Run in the cloud using Amazon Elastic MapReduce (EMR)\n",
    "- Run in the cloud using Google Cloud Dataproc (Dataproc)\n",
    "- Easily run Spark jobs on EMR or your own Hadoop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to write Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./utils/mrjob_ratings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# test mrjob script locally \n",
    "# only Python works with no YARN, Hadoop, HDFS etc.\n",
    "\n",
    "python ./utils/mrjob_ratings.py \\\n",
    "    ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan/input_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put local file to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/test_hdfs.txt /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# now let's run mrjob script within Hadoop \n",
    "# NOTE: python3 is used, it os a feature of mrjob\n",
    "\n",
    "python3 ./utils/mrjob_ratings.py \\\n",
    "    --python-bin /opt/conda/bin/python3 \\\n",
    "    -r hadoop hdfs:///jovyan/input_csv/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traces of mrjob in the HDFS (logs, outputs etc.)\n",
    "!hdfs dfs -ls /user/jovyan/tmp/mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. MRJob for crypto currencies analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cryptocurrency Price & Market Data dataset](https://www.kaggle.com/datasets/thedevastator/cryptocurrency-price-market-data) provides the insights into the cryptocurrency markets. It collects important data points such as:\n",
    "- name of the cryptocurrency\n",
    "- symbol\n",
    "- price\n",
    "- hourly and daily change trends\n",
    "- 24 hour volume traded\n",
    "- market capitalization\n",
    "\n",
    "Our goal will be to find top-10 `24 hour volume traded` crypto currencies with the help of `mrjob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ~/__DATA/IBDT_Spring_2023/topic_3/coin_gecko_2022-03-17.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir {WORK_DIR}/input_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/coin_gecko_2022-03-17.csv {WORK_DIR}/input_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/input_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# test mrjob script locally \n",
    "# only Python works with no YARN, Hadoop, HDFS etc.\n",
    "\n",
    "python ./utils/mrjob_crypto.py \\\n",
    "    ~/__DATA/IBDT_Spring_2023/topic_3/coin_gecko_2022-03-17.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# now let's run mrjob script within Hadoop \n",
    "# NOTE: python3 is used, it is a feature of mrjob\n",
    "\n",
    "python3 ./utils/mrjob_crypto.py \\\n",
    "    --python-bin /opt/conda/bin/python3 \\\n",
    "    -r hadoop hdfs:///jovyan/input_crypto/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [Video Game Sales dataset](https://www.kaggle.com/datasets/gregorut/videogamesales) that contains a list of video games with sales greater than 100,000 copies\n",
    "\n",
    "Fields of the dataset include:\n",
    "- Rank - Ranking of overall sales\n",
    "- Name - The games name\n",
    "- Platform - Platform of the games release (i.e. PC,PS4, etc.)\n",
    "- Year - Year of the game's release\n",
    "- Genre - Genre of the game\n",
    "- Publisher - Publisher of the game\n",
    "- NA_Sales - Sales in North America (in millions)\n",
    "- EU_Sales - Sales in Europe (in millions)\n",
    "- JP_Sales - Sales in Japan (in millions)\n",
    "- Other_Sales - Sales in the rest of the world (in millions)\n",
    "- Global_Sales - Total worldwide sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the Video Game Sales dataset loaded\n",
    "!ls ~/__DATA/IBDT_Spring_2023/topic_3/vgsales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your home assignment for this part is:\n",
    "1. Take the `vgsales.csv` and load it to HDFS\n",
    "2. Count the number of video games by the platform (field `Platform` in the file)\n",
    "3. Find top-5 videp games by sales in Japan (field `JP_Sales`)\n",
    "\n",
    "Please use `mrjob` library count for the tasks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
