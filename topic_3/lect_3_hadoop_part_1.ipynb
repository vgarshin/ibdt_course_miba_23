{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Modern Technologies course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC 3: Hadoop and MapReduce practice\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need port only for Web UI\n",
    "YARN_PORT = 8088\n",
    "\n",
    "# working directory for default user `jovyan`\n",
    "# `/jovyan/home` for the Jupyter \n",
    "# and `/jovyan` for the Hadoop environment\n",
    "WORK_DIR = '/jovyan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HDFS commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help is all you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\r\n",
      "\t[-appendToFile <localsrc> ... <dst>]\r\n",
      "\t[-cat [-ignoreCrc] <src> ...]\r\n",
      "\t[-checksum <src> ...]\r\n",
      "\t[-chgrp [-R] GROUP PATH...]\r\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\r\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\r\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\r\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\r\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\r\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\r\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\r\n",
      "\t[-df [-h] [<path> ...]]\r\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\r\n",
      "\t[-expunge [-immediate]]\r\n",
      "\t[-find <path> ... <expression> ...]\r\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-getfacl [-R] <path>]\r\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\r\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\r\n",
      "\t[-head <file>]\r\n",
      "\t[-help [cmd ...]]\r\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\r\n",
      "\t[-mkdir [-p] <path> ...]\r\n",
      "\t[-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\r\n",
      "\t[-moveToLocal <src> <localdst>]\r\n",
      "\t[-mv <src> ... <dst>]\r\n",
      "\t[-put [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\r\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\r\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\r\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\r\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\r\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\r\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\r\n",
      "\t[-stat [format] <path> ...]\r\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\r\n",
      "\t[-test -[defswrz] <path>]\r\n",
      "\t[-text [-ignoreCrc] <src> ...]\r\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\r\n",
      "\t[-touchz <path> ...]\r\n",
      "\t[-truncate [-w] <length> <path> ...]\r\n",
      "\t[-usage [cmd ...]]\r\n",
      "\r\n",
      "-appendToFile <localsrc> ... <dst> :\r\n",
      "  Appends the contents of all the given local files to the given dst file. The dst\r\n",
      "  file will be created if it does not exist. If <localSrc> is -, then the input is\r\n",
      "  read from stdin.\r\n",
      "\r\n",
      "-cat [-ignoreCrc] <src> ... :\r\n",
      "  Fetch all files that match the file pattern <src> and display their content on\r\n",
      "  stdout.\r\n",
      "\r\n",
      "-checksum <src> ... :\r\n",
      "  Dump checksum information for files that match the file pattern <src> to stdout.\r\n",
      "  Note that this requires a round-trip to a datanode storing each block of the\r\n",
      "  file, and thus is not efficient to run on a large number of files. The checksum\r\n",
      "  of a file depends on its content, block size and the checksum algorithm and\r\n",
      "  parameters used for creating the file.\r\n",
      "\r\n",
      "-chgrp [-R] GROUP PATH... :\r\n",
      "  This is equivalent to -chown ... :GROUP ...\r\n",
      "\r\n",
      "-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH... :\r\n",
      "  Changes permissions of a file. This works similar to the shell's chmod command\r\n",
      "  with a few exceptions.\r\n",
      "                                                                                 \r\n",
      "  -R           modifies the files recursively. This is the only option currently \r\n",
      "               supported.                                                        \r\n",
      "  <MODE>       Mode is the same as mode used for the shell's command. The only   \r\n",
      "               letters recognized are 'rwxXt', e.g. +t,a+r,g-w,+rwx,o=r.         \r\n",
      "  <OCTALMODE>  Mode specifed in 3 or 4 digits. If 4 digits, the first may be 1 or\r\n",
      "               0 to turn the sticky bit on or off, respectively.  Unlike the     \r\n",
      "               shell command, it is not possible to specify only part of the     \r\n",
      "               mode, e.g. 754 is same as u=rwx,g=rx,o=r.                         \r\n",
      "  \r\n",
      "  If none of 'augo' is specified, 'a' is assumed and unlike the shell command, no\r\n",
      "  umask is applied.\r\n",
      "\r\n",
      "-chown [-R] [OWNER][:[GROUP]] PATH... :\r\n",
      "  Changes owner and group of a file. This is similar to the shell's chown command\r\n",
      "  with a few exceptions.\r\n",
      "                                                                                 \r\n",
      "  -R  modifies the files recursively. This is the only option currently          \r\n",
      "      supported.                                                                 \r\n",
      "  \r\n",
      "  If only the owner or group is specified, then only the owner or group is\r\n",
      "  modified. The owner and group names may only consist of digits, alphabet, and\r\n",
      "  any of [-_./@a-zA-Z0-9]. The names are case sensitive.\r\n",
      "  \r\n",
      "  WARNING: Avoid using '.' to separate user name and group though Linux allows it.\r\n",
      "  If user names have dots in them and you are using local file system, you might\r\n",
      "  see surprising results since the shell command 'chown' is used for local files.\r\n",
      "\r\n",
      "-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst> :\r\n",
      "  Identical to the -put command.\r\n",
      "\r\n",
      "-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :\r\n",
      "  Identical to the -get command.\r\n",
      "\r\n",
      "-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ... :\r\n",
      "  Count the number of directories, files and bytes under the paths\r\n",
      "  that match the specified file pattern.  The output columns are:\r\n",
      "  DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\r\n",
      "  or, with the -q option:\r\n",
      "  QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA\r\n",
      "        DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\r\n",
      "  The -h option shows file sizes in human readable format.\r\n",
      "  The -v option displays a header line.\r\n",
      "  The -x option excludes snapshots from being calculated. \r\n",
      "  The -t option displays quota by storage types.\r\n",
      "  It should be used with -q or -u option, otherwise it will be ignored.\r\n",
      "  If a comma-separated list of storage types is given after the -t option, \r\n",
      "  it displays the quota and usage for the specified types. \r\n",
      "  Otherwise, it displays the quota and usage for all the storage \r\n",
      "  types that support quota. The list of possible storage types(case insensitive):\r\n",
      "  ram_disk, ssd, disk and archive.\r\n",
      "  It can also pass the value '', 'all' or 'ALL' to specify all the storage types.\r\n",
      "  The -u option shows the quota and \r\n",
      "  the usage against the quota without the detailed content summary.The -e option\r\n",
      "  shows the erasure coding policy.\r\n",
      "\r\n",
      "-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst> :\r\n",
      "  Copy files that match the file pattern <src> to a destination.  When copying\r\n",
      "  multiple files, the destination must be a directory. Passing -p preserves status\r\n",
      "  [topax] (timestamps, ownership, permission, ACLs, XAttr). If -p is specified\r\n",
      "  with no <arg>, then preserves timestamps, ownership, permission. If -pa is\r\n",
      "  specified, then preserves permission also because ACL is a super-set of\r\n",
      "  permission. Passing -f overwrites the destination if it already exists. raw\r\n",
      "  namespace extended attributes are preserved if (1) they are supported (HDFS\r\n",
      "  only) and, (2) all of the source and target pathnames are in the /.reserved/raw\r\n",
      "  hierarchy. raw namespace xattr preservation is determined solely by the presence\r\n",
      "  (or absence) of the /.reserved/raw prefix and not by the -p option. Passing -d\r\n",
      "  will skip creation of temporary file(<dst>._COPYING_).\r\n",
      "\r\n",
      "-createSnapshot <snapshotDir> [<snapshotName>] :\r\n",
      "  Create a snapshot on a directory\r\n",
      "\r\n",
      "-deleteSnapshot <snapshotDir> <snapshotName> :\r\n",
      "  Delete a snapshot from a directory\r\n",
      "\r\n",
      "-df [-h] [<path> ...] :\r\n",
      "  Shows the capacity, free and used space of the filesystem. If the filesystem has\r\n",
      "  multiple partitions, and no path to a particular partition is specified, then\r\n",
      "  the status of the root partitions will be shown.\r\n",
      "                                                                                 \r\n",
      "  -h  Formats the sizes of files in a human-readable fashion rather than a number\r\n",
      "      of bytes.                                                                  \r\n",
      "\r\n",
      "-du [-s] [-h] [-v] [-x] <path> ... :\r\n",
      "  Show the amount of space, in bytes, used by the files that match the specified\r\n",
      "  file pattern. The following flags are optional:\r\n",
      "                                                                                 \r\n",
      "  -s  Rather than showing the size of each individual file that matches the      \r\n",
      "      pattern, shows the total (summary) size.                                   \r\n",
      "  -h  Formats the sizes of files in a human-readable fashion rather than a number\r\n",
      "      of bytes.                                                                  \r\n",
      "  -v  option displays a header line.                                             \r\n",
      "  -x  Excludes snapshots from being counted.                                     \r\n",
      "  \r\n",
      "  Note that, even without the -s option, this only shows size summaries one level\r\n",
      "  deep into a directory.\r\n",
      "  \r\n",
      "  The output is in the form \r\n",
      "  \tsize\tdisk space consumed\tname(full path)\r\n",
      "\r\n",
      "-expunge [-immediate] :\r\n",
      "  Delete files from the trash that are older than the retention threshold\r\n",
      "\r\n",
      "-find <path> ... <expression> ... :\r\n",
      "  Finds all files that match the specified expression and\r\n",
      "  applies selected actions to them. If no <path> is specified\r\n",
      "  then defaults to the current working directory. If no\r\n",
      "  expression is specified then defaults to -print.\r\n",
      "  \r\n",
      "  The following primary expressions are recognised:\r\n",
      "    -name pattern\r\n",
      "    -iname pattern\r\n",
      "      Evaluates as true if the basename of the file matches the\r\n",
      "      pattern using standard file system globbing.\r\n",
      "      If -iname is used then the match is case insensitive.\r\n",
      "  \r\n",
      "    -print\r\n",
      "    -print0\r\n",
      "      Always evaluates to true. Causes the current pathname to be\r\n",
      "      written to standard output followed by a newline. If the -print0\r\n",
      "      expression is used then an ASCII NULL character is appended rather\r\n",
      "      than a newline.\r\n",
      "  \r\n",
      "  The following operators are recognised:\r\n",
      "    expression -a expression\r\n",
      "    expression -and expression\r\n",
      "    expression expression\r\n",
      "      Logical AND operator for joining two expressions. Returns\r\n",
      "      true if both child expressions return true. Implied by the\r\n",
      "      juxtaposition of two expressions and so does not need to be\r\n",
      "      explicitly specified. The second expression will not be\r\n",
      "      applied if the first fails.\r\n",
      "\r\n",
      "-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :\r\n",
      "  Copy files that match the file pattern <src> to the local name.  <src> is kept. \r\n",
      "  When copying multiple files, the destination must be a directory. Passing -f\r\n",
      "  overwrites the destination if it already exists and -p preserves access and\r\n",
      "  modification times, ownership and the mode.\r\n",
      "\r\n",
      "-getfacl [-R] <path> :\r\n",
      "  Displays the Access Control Lists (ACLs) of files and directories. If a\r\n",
      "  directory has a default ACL, then getfacl also displays the default ACL.\r\n",
      "                                                                  \r\n",
      "  -R      List the ACLs of all files and directories recursively. \r\n",
      "  <path>  File or directory to list.                              \r\n",
      "\r\n",
      "-getfattr [-R] {-n name | -d} [-e en] <path> :\r\n",
      "  Displays the extended attribute names and values (if any) for a file or\r\n",
      "  directory.\r\n",
      "                                                                                 \r\n",
      "  -R             Recursively list the attributes for all files and directories.  \r\n",
      "  -n name        Dump the named extended attribute value.                        \r\n",
      "  -d             Dump all extended attribute values associated with pathname.    \r\n",
      "  -e <encoding>  Encode values after retrieving them.Valid encodings are \"text\", \r\n",
      "                 \"hex\", and \"base64\". Values encoded as text strings are enclosed\r\n",
      "                 in double quotes (\"), and values encoded as hexadecimal and     \r\n",
      "                 base64 are prefixed with 0x and 0s, respectively.               \r\n",
      "  <path>         The file or directory.                                          \r\n",
      "\r\n",
      "-getmerge [-nl] [-skip-empty-file] <src> <localdst> :\r\n",
      "  Get all the files in the directories that match the source file pattern and\r\n",
      "  merge and sort them to only one file on local fs. <src> is kept.\r\n",
      "                                                                     \r\n",
      "  -nl               Add a newline character at the end of each file. \r\n",
      "  -skip-empty-file  Do not add new line character for empty file.    \r\n",
      "\r\n",
      "-head <file> :\r\n",
      "  Show the first 1KB of the file.\r\n",
      "\r\n",
      "-help [cmd ...] :\r\n",
      "  Displays help for given command or all commands if none is specified.\r\n",
      "\r\n",
      "-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...] :\r\n",
      "  List the contents that match the specified file pattern. If path is not\r\n",
      "  specified, the contents of /user/<currentUser> will be listed. For a directory a\r\n",
      "  list of its direct children is returned (unless -d option is specified).\r\n",
      "  \r\n",
      "  Directory entries are of the form:\r\n",
      "  \tpermissions - userId groupId sizeOfDirectory(in bytes)\r\n",
      "  modificationDate(yyyy-MM-dd HH:mm) directoryName\r\n",
      "  \r\n",
      "  and file entries are of the form:\r\n",
      "  \tpermissions numberOfReplicas userId groupId sizeOfFile(in bytes)\r\n",
      "  modificationDate(yyyy-MM-dd HH:mm) fileName\r\n",
      "  \r\n",
      "    -C  Display the paths of files and directories only.\r\n",
      "    -d  Directories are listed as plain files.\r\n",
      "    -h  Formats the sizes of files in a human-readable fashion\r\n",
      "        rather than a number of bytes.\r\n",
      "    -q  Print ? instead of non-printable characters.\r\n",
      "    -R  Recursively list the contents of directories.\r\n",
      "    -t  Sort files by modification time (most recent first).\r\n",
      "    -S  Sort files by size.\r\n",
      "    -r  Reverse the order of the sort.\r\n",
      "    -u  Use time of last access instead of modification for\r\n",
      "        display and sorting.\r\n",
      "    -e  Display the erasure coding policy of files and directories.\r\n",
      "\r\n",
      "-mkdir [-p] <path> ... :\r\n",
      "  Create a directory in specified location.\r\n",
      "                                                  \r\n",
      "  -p  Do not fail if the directory already exists \r\n",
      "\r\n",
      "-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst> :\r\n",
      "  Same as -put, except that the source is deleted after it's copied\r\n",
      "  and -t option has not yet implemented.\r\n",
      "\r\n",
      "-moveToLocal <src> <localdst> :\r\n",
      "  Not implemented yet\r\n",
      "\r\n",
      "-mv <src> ... <dst> :\r\n",
      "  Move files that match the specified file pattern <src> to a destination <dst>. \r\n",
      "  When moving multiple files, the destination must be a directory.\r\n",
      "\r\n",
      "-put [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst> :\r\n",
      "  Copy files from the local file system into fs. Copying fails if the file already\r\n",
      "  exists, unless the -f flag is given.\r\n",
      "  Flags:\r\n",
      "                                                                                 \r\n",
      "  -p                 Preserves timestamps, ownership and the mode.               \r\n",
      "  -f                 Overwrites the destination if it already exists.            \r\n",
      "  -t <thread count>  Number of threads to be used, default is 1.                 \r\n",
      "  -l                 Allow DataNode to lazily persist the file to disk. Forces   \r\n",
      "                     replication factor of 1. This flag will result in reduced   \r\n",
      "                     durability. Use with care.                                  \r\n",
      "  -d                 Skip creation of temporary file(<dst>._COPYING_).           \r\n",
      "\r\n",
      "-renameSnapshot <snapshotDir> <oldName> <newName> :\r\n",
      "  Rename a snapshot from oldName to newName\r\n",
      "\r\n",
      "-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :\r\n",
      "  Delete all files that match the specified file pattern. Equivalent to the Unix\r\n",
      "  command \"rm <src>\"\r\n",
      "                                                                                 \r\n",
      "  -f          If the file does not exist, do not display a diagnostic message or \r\n",
      "              modify the exit status to reflect an error.                        \r\n",
      "  -[rR]       Recursively deletes directories.                                   \r\n",
      "  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  \r\n",
      "  -safely     option requires safety confirmation, if enabled, requires          \r\n",
      "              confirmation before deleting large directory with more than        \r\n",
      "              <hadoop.shell.delete.limit.num.files> files. Delay is expected when\r\n",
      "              walking over large directory recursively to count the number of    \r\n",
      "              files to be deleted before the confirmation.                       \r\n",
      "\r\n",
      "-rmdir [--ignore-fail-on-non-empty] <dir> ... :\r\n",
      "  Removes the directory entry specified by each directory argument, provided it is\r\n",
      "  empty.\r\n",
      "\r\n",
      "-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>] :\r\n",
      "  Sets Access Control Lists (ACLs) of files and directories.\r\n",
      "  Options:\r\n",
      "                                                                                 \r\n",
      "  -b          Remove all but the base ACL entries. The entries for user, group   \r\n",
      "              and others are retained for compatibility with permission bits.    \r\n",
      "  -k          Remove the default ACL.                                            \r\n",
      "  -R          Apply operations to all files and directories recursively.         \r\n",
      "  -m          Modify ACL. New entries are added to the ACL, and existing entries \r\n",
      "              are retained.                                                      \r\n",
      "  -x          Remove specified ACL entries. Other ACL entries are retained.      \r\n",
      "  --set       Fully replace the ACL, discarding all existing entries. The        \r\n",
      "              <acl_spec> must include entries for user, group, and others for    \r\n",
      "              compatibility with permission bits. If the ACL spec contains only  \r\n",
      "              access entries, then the existing default entries are retained. If \r\n",
      "              the ACL spec contains only default entries, then the existing      \r\n",
      "              access entries are retained. If the ACL spec contains both access  \r\n",
      "              and default entries, then both are replaced.                       \r\n",
      "  <acl_spec>  Comma separated list of ACL entries.                               \r\n",
      "  <path>      File or directory to modify.                                       \r\n",
      "\r\n",
      "-setfattr {-n name [-v value] | -x name} <path> :\r\n",
      "  Sets an extended attribute name and value for a file or directory.\r\n",
      "                                                                                 \r\n",
      "  -n name   The extended attribute name.                                         \r\n",
      "  -v value  The extended attribute value. There are three different encoding     \r\n",
      "            methods for the value. If the argument is enclosed in double quotes, \r\n",
      "            then the value is the string inside the quotes. If the argument is   \r\n",
      "            prefixed with 0x or 0X, then it is taken as a hexadecimal number. If \r\n",
      "            the argument begins with 0s or 0S, then it is taken as a base64      \r\n",
      "            encoding.                                                            \r\n",
      "  -x name   Remove the extended attribute.                                       \r\n",
      "  <path>    The file or directory.                                               \r\n",
      "\r\n",
      "-setrep [-R] [-w] <rep> <path> ... :\r\n",
      "  Set the replication level of a file. If <path> is a directory then the command\r\n",
      "  recursively changes the replication factor of all files under the directory tree\r\n",
      "  rooted at <path>. The EC files will be ignored here.\r\n",
      "                                                                                 \r\n",
      "  -w  It requests that the command waits for the replication to complete. This   \r\n",
      "      can potentially take a very long time.                                     \r\n",
      "  -R  It is accepted for backwards compatibility. It has no effect.              \r\n",
      "\r\n",
      "-stat [format] <path> ... :\r\n",
      "  Print statistics about the file/directory at <path>\r\n",
      "  in the specified format. Format accepts permissions in\r\n",
      "  octal (%a) and symbolic (%A), filesize in\r\n",
      "  bytes (%b), type (%F), group name of owner (%g),\r\n",
      "  name (%n), block size (%o), replication (%r), user name\r\n",
      "  of owner (%u), access date (%x, %X).\r\n",
      "  modification date (%y, %Y).\r\n",
      "  %x and %y show UTC date as \"yyyy-MM-dd HH:mm:ss\" and\r\n",
      "  %X and %Y show milliseconds since January 1, 1970 UTC.\r\n",
      "  If the format is not specified, %y is used by default.\r\n",
      "\r\n",
      "-tail [-f] [-s <sleep interval>] <file> :\r\n",
      "  Show the last 1KB of the file.\r\n",
      "                                                                               \r\n",
      "  -f  Shows appended data as the file grows.                                   \r\n",
      "  -s  With -f , defines the sleep interval between iterations in milliseconds. \r\n",
      "\r\n",
      "-test -[defswrz] <path> :\r\n",
      "  Answer various questions about <path>, with result via exit status.\r\n",
      "    -d  return 0 if <path> is a directory.\r\n",
      "    -e  return 0 if <path> exists.\r\n",
      "    -f  return 0 if <path> is a file.\r\n",
      "    -s  return 0 if file <path> is greater         than zero bytes in size.\r\n",
      "    -w  return 0 if file <path> exists         and write permission is granted.\r\n",
      "    -r  return 0 if file <path> exists         and read permission is granted.\r\n",
      "    -z  return 0 if file <path> is         zero bytes in size, else return 1.\r\n",
      "\r\n",
      "-text [-ignoreCrc] <src> ... :\r\n",
      "  Takes a source file and outputs the file in text format.\r\n",
      "  The allowed formats are zip and TextRecordInputStream and Avro.\r\n",
      "\r\n",
      "-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ... :\r\n",
      "  Updates the access and modification times of the file specified by the <path> to\r\n",
      "  the current time. If the file does not exist, then a zero length file is created\r\n",
      "  at <path> with current time as the timestamp of that <path>.\r\n",
      "  -a Change only the access time \r\n",
      "  -m Change only the modification time \r\n",
      "  -t TIMESTAMP Use specified timestamp (in format yyyyMMddHHmmss) instead of\r\n",
      "  current time \r\n",
      "  -c Do not create any files\r\n",
      "\r\n",
      "-touchz <path> ... :\r\n",
      "  Creates a file of zero length at <path> with current time as the timestamp of\r\n",
      "  that <path>. An error is returned if the file exists with non-zero length\r\n",
      "\r\n",
      "-truncate [-w] <length> <path> ... :\r\n",
      "  Truncate all files that match the specified file pattern to the specified\r\n",
      "  length.\r\n",
      "                                                                                 \r\n",
      "  -w  Requests that the command wait for block recovery to complete, if          \r\n",
      "      necessary.                                                                 \r\n",
      "\r\n",
      "-usage [cmd ...] :\r\n",
      "  Displays the usage for given command or all commands if none is specified.\r\n",
      "\r\n",
      "Generic options supported are:\r\n",
      "-conf <configuration file>        specify an application configuration file\r\n",
      "-D <property=value>               define a value for a given property\r\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\r\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\r\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\r\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\r\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\r\n",
      "\r\n",
      "The general command line syntax is:\r\n",
      "command [genericOptions] [commandOptions]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigation through HDFS is available with `hdfs dfs` [commands](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html) which are quite simular to Unix shell navigation (`ls`, `cat`, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - hadoop supergroup           0 2023-05-02 15:25 /hbase\n",
      "drwxr-xr-x   - jovyan hadoopusers          0 2023-05-02 15:24 /jovyan\n",
      "drwxrwxrwx   - hadoop supergroup           0 2023-05-02 15:24 /tmp\n",
      "drwxr-xr-x   - jovyan hadoopusers          0 2023-05-02 15:25 /user\n"
     ]
    }
   ],
   "source": [
    "# list root directory\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list directory\n",
    "!hdfs dfs -ls /jovyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or with `WORK_DIR` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list working directory '/jovyan'\n",
    "# NOTE: variable WORK_DIR='/jovyan' used in braces\n",
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Put and get files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put an arbitary file to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16004\r\n",
      "drwxrwxrwx 2 root root        0 Mar  1 13:21 .\r\n",
      "drwxrwxrwx 2 root root        0 Feb 10 12:43 ..\r\n",
      "-rw-rw-rw- 1 root root     9161 Feb 22 09:29 big_data_trends_2021.txt\r\n",
      "-rw-rw-rw- 1 root root    11836 Feb 22 09:28 big_data_trends_2023.txt\r\n",
      "-rw-rw-rw- 1 root root    43234 Feb 28 15:52 coin_gecko_2022-03-17.csv\r\n",
      "-rw-rw-rw- 1 root root 14966021 Feb 28 14:51 Hotel_Reviews.csv\r\n",
      "drwxrwxrwx 2 root root        0 Feb 22 08:06 .ipynb_checkpoints\r\n",
      "-rw-rw-rw- 1 root root       55 Feb 22 08:06 test_hdfs.txt\r\n",
      "-rw-rw-rw- 1 root root  1355781 Mar  1 13:21 vgsales.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ~/__DATA/IBDT_Spring_2023/topic_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put local file to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/test_hdfs.txt {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 jovyan hadoopusers         55 2023-05-02 15:31 /jovyan/test_hdfs.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a file.\r\n",
      "I am in the HDFS now and feel good."
     ]
    }
   ],
   "source": [
    "# look at the file's content\n",
    "!hdfs dfs -cat {WORK_DIR}/test_hdfs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folders and move files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir {WORK_DIR}/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan hadoopusers         55 2023-05-02 15:31 /jovyan/test_hdfs.txt\r\n",
      "drwxr-xr-x   - jovyan hadoopusers          0 2023-05-02 15:31 /jovyan/texts\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv {WORK_DIR}/test_hdfs.txt {WORK_DIR}/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - jovyan hadoopusers          0 2023-05-02 15:31 /jovyan/texts\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 jovyan hadoopusers         55 2023-05-02 15:31 /jovyan/texts/test_hdfs.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a file.\r\n",
      "I am in the HDFS now and feel good."
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {WORK_DIR}/texts/test_hdfs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get files back from `HDFS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get: `test_hdfs.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -get {WORK_DIR}/texts/test_hdfs.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Something useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_dirs(path, filter_str=''):\n",
    "    \"\"\"\n",
    "    Returns files in path provided as a list. \n",
    "    File names may be filtered by `filter_str` parameter,\n",
    "    e.g. `filter_str='csv'` will display only `csv` files.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-ls', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    dirs = out.decode('utf-8').split('\\n')\n",
    "    dirs = list(filter(lambda x: filter_str in x, dirs))\n",
    "    dirs = list(map(lambda x: x.split(' ')[-1], dirs))\n",
    "    return dirs\n",
    "\n",
    "def file_content(path):\n",
    "    \"\"\"\n",
    "    Returns content of the file.\n",
    "    Similar to `cat` command.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-cat', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    return out.decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use function defined above\n",
    "hdfs_dirs(WORK_DIR, 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/jovyan/texts/test_hdfs.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_dirs(WORK_DIR + '/texts', 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_dirs(WORK_DIR + '/texts', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am a file.\\nI am in the HDFS now and feel good.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the content of the 'telecom_churn.csv' file\n",
    "content = file_content(f'{WORK_DIR}/texts/test_hdfs.txt')\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MapReduce intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. WordCount with Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordCount` is a simple application that counts the number of occurrences of each word in a given input set. For this demo ready `jar` package is used.\n",
    "\n",
    "First let's copy files to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 jovyan hadoopusers       9161 2023-05-02 15:31 /jovyan/input/big_data_trends_2021.txt\n",
      "-rw-r--r--   1 jovyan hadoopusers      11836 2023-05-02 15:31 /jovyan/input/big_data_trends_2023.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# create input directory on HDFS\n",
    "hdfs dfs -mkdir -p ${work_dir}/input\n",
    "\n",
    "# put files to HDFS\n",
    "hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/big_data_* ${work_dir}/input\n",
    "hdfs dfs -ls ${work_dir}/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a map-reduce job and enjoy long logs output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 15:31:29,155 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-05-02 15:31:29,485 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1683041087371_0001\n",
      "2023-05-02 15:31:29,691 INFO input.FileInputFormat: Total input files to process : 2\n",
      "2023-05-02 15:31:29,758 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-05-02 15:31:30,260 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1683041087371_0001\n",
      "2023-05-02 15:31:30,262 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-05-02 15:31:30,394 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-05-02 15:31:30,394 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-05-02 15:31:30,607 INFO impl.YarnClientImpl: Submitted application application_1683041087371_0001\n",
      "2023-05-02 15:31:30,658 INFO mapreduce.Job: The url to track the job: http://0.0.0.0:60000/proxy/application_1683041087371_0001/\n",
      "2023-05-02 15:31:30,659 INFO mapreduce.Job: Running job: job_1683041087371_0001\n",
      "2023-05-02 15:31:36,735 INFO mapreduce.Job: Job job_1683041087371_0001 running in uber mode : false\n",
      "2023-05-02 15:31:36,736 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-05-02 15:31:40,778 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-05-02 15:31:45,801 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-05-02 15:31:46,811 INFO mapreduce.Job: Job job_1683041087371_0001 completed successfully\n",
      "2023-05-02 15:31:46,892 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=18105\n",
      "\t\tFILE: Number of bytes written=741452\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21231\n",
      "\t\tHDFS: Number of bytes written=11711\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4048\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1780\n",
      "\t\tTotal time spent by all map tasks (ms)=4048\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1780\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4048\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1780\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4145152\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1822720\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=117\n",
      "\t\tMap output records=3155\n",
      "\t\tMap output bytes=33588\n",
      "\t\tMap output materialized bytes=18111\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=3155\n",
      "\t\tCombine output records=1278\n",
      "\t\tReduce input groups=1130\n",
      "\t\tReduce shuffle bytes=18111\n",
      "\t\tReduce input records=1278\n",
      "\t\tReduce output records=1130\n",
      "\t\tSpilled Records=2556\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=146\n",
      "\t\tCPU time spent (ms)=1380\n",
      "\t\tPhysical memory (bytes) snapshot=737615872\n",
      "\t\tVirtual memory (bytes) snapshot=7632228352\n",
      "\t\tTotal committed heap usage (bytes)=700973056\n",
      "\t\tPeak Map Physical memory (bytes)=280829952\n",
      "\t\tPeak Map Virtual memory (bytes)=2541023232\n",
      "\t\tPeak Reduce Physical memory (bytes)=177025024\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2550325248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=20997\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=11711\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# delete directory if exists\n",
    "#hdfs dfs -rm -r ${work_dir}/output\n",
    "\n",
    "# run wordcount\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar wordcount \\\n",
    "    ${work_dir}/input ${work_dir}/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan hadoopusers          0 2023-05-02 15:31 /jovyan/output/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan hadoopusers      11711 2023-05-02 15:31 /jovyan/output/part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat {WORK_DIR}/output/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wordcount output:\n",
      "(09\t1\n",
      "(D&A)\t1\n",
      "(IoT)\t1\n",
      "(ML).\t1\n",
      "(balanced\t1\n",
      "(context,\t1\n",
      "(real-time)\t1\n",
      "(see\t2\n",
      "(small\t1\n",
      "(velocity)\t1\n",
      "(veracity),\t1\n",
      "(wide\t1\n",
      "-\t2\n",
      "--\t8\n",
      "05\t1\n",
      "1).\t1\n",
      "1.\t2\n",
      "10\t2\n",
      "16\t1\n",
      "193\t1\n",
      "2.\t1\n",
      "2020,\t1\n",
      "2020.\t1\n",
      "2021\t1\n",
      "2021)\t1\n",
      "2021:\t2\n",
      "2023\t4\n",
      "2023.\t1\n",
      "2025,\t1\n",
      "3.\t1\n",
      "360-degree\t2\n",
      "4\t1\n",
      "4.\t1\n",
      "6\t1\n",
      "63%\t1\n",
      "70%\t1\n",
      "AI\t13\n",
      "AI,\t5\n",
      "AI-enabled\t2\n",
      "AI.\t1\n",
      "AWS,\t1\n",
      "Actions:\t1\n",
      "Additional\t1\n",
      "Adoption\t1\n",
      "Advanced\t1\n",
      "Analysis\t1\n",
      "Analytics\t6\n",
      "Analytics,\t1\n",
      "Android\t1\n",
      "Apple\t1\n",
      "As\t4\n",
      "Assumption\t1\n",
      "Big\t7\n",
      "But\t1\n",
      "By\t4\n",
      "COVID-19\t1\n",
      "Changes\t1\n",
      "Choudhary,\t1\n",
      "Clougherty\t1\n",
      "Cognilytica\t1\n",
      "Collectively,\t1\n",
      "D&A\t2\n",
      "Data\t12\n",
      "DataOps\t2\n",
      "DataOps,\t1\n",
      "Dealing\t1\n",
      "Description:\t1\n",
      "Distributed\t1\n",
      "Due\t1\n",
      "Each\t1\n",
      "Edge\t2\n",
      "Emerging\t1\n",
      "Enrich\t1\n",
      "Enterprise\t2\n",
      "Enterprises\t3\n",
      "Evidence\t1\n",
      "Explore\t1\n",
      "Extend\t1\n",
      "Farhan\t1\n",
      "February\t1\n",
      "Figure\t2\n",
      "Fitbit,\t1\n",
      "For\t2\n",
      "Four\t1\n",
      "From\t4\n",
      "G00738992\t1\n",
      "Gartner\t2\n",
      "Google\t1\n",
      "Google,\t1\n",
      "Group\t2\n",
      "Hadoop\t1\n",
      "Hamer,\t2\n",
      "Hare\t1\n",
      "Hare,\t1\n",
      "Here\t1\n",
      "Here's\t1\n",
      "IBM\t1\n",
      "ID\t1\n",
      "IT\t1\n",
      "Implications:\t1\n",
      "In\t9\n",
      "Increasingly,\t1\n",
      "Indeed,\t1\n",
      "Initiatives:Data\t1\n",
      "Internet\t1\n",
      "IoT\t2\n",
      "It\t1\n",
      "Jan\t1\n",
      "January\t1\n",
      "Jim\t2\n",
      "Jones\t1\n",
      "Know\t1\n",
      "Last\t1\n",
      "Likewise,\t1\n",
      "Link\t1\n",
      "Lydia\t1\n",
      "ML\t1\n",
      "Machine\t1\n",
      "Many\t1\n",
      "March\t1\n",
      "Microsoft,\t1\n",
      "More\t1\n",
      "Moreover,\t1\n",
      "Much\t2\n",
      "Mutual\t1\n",
      "Need\t1\n",
      "New\t2\n",
      "No\t1\n",
      "Non-database\t1\n",
      "November\t1\n",
      "One\t1\n",
      "Opportunities\t1\n",
      "Oracle\t1\n",
      "Organizations\t1\n",
      "Other\t3\n",
      "Overview\t1\n",
      "People\t1\n",
      "Perspectives\t1\n",
      "Pieter\t2\n",
      "Planning\t1\n",
      "Potential\t1\n",
      "Practices\t1\n",
      "Previously\t1\n",
      "Programs\t1\n",
      "Published\t1\n",
      "Published:\t1\n",
      "Rather\t2\n",
      "Recommendations\t1\n",
      "Ronald\t2\n",
      "Schmelzer\t1\n",
      "Schmelzer,\t1\n",
      "Shubhangi\t1\n",
      "Since\t1\n",
      "Small\t8\n",
      "Some\t2\n",
      "Spark,\t1\n",
      "Strategic\t1\n",
      "Strategy\t2\n",
      "Summary\t1\n",
      "Taken\t1\n",
      "TechTarget's\t1\n",
      "Techniques\t1\n",
      "That\t2\n",
      "The\t20\n",
      "There\t1\n",
      "These\t4\n",
      "They\t1\n",
      "Things\t1\n",
      "This\t7\n",
      "This,\t2\n",
      "To\t1\n",
      "Top\t5\n",
      "Translation:\t1\n",
      "Trending:\t1\n",
      "Trends\t4\n",
      "V's\t2\n",
      "Vashisth,\t1\n",
      "Voice\t1\n",
      "Volume\t1\n",
      "Watch\t1\n",
      "What\t1\n",
      "Why\t1\n",
      "Wide\t4\n",
      "With\t1\n",
      "X\t5\n",
      "Year\t1\n",
      "You\t1\n",
      "a\t46\n",
      "ability\t1\n",
      "able\t2\n",
      "about\t6\n",
      "accelerate.\t1\n",
      "accuracy\t1\n",
      "accurate\t2\n",
      "achieved\t1\n",
      "across\t5\n",
      "adaptive\t1\n",
      "adaptive,\t2\n",
      "addition\t1\n",
      "addition,\t3\n",
      "address\t1\n",
      "address.\t1\n",
      "advanced\t6\n",
      "advances\t1\n",
      "advancing\t1\n",
      "advantages\t1\n",
      "aggregate\t1\n",
      "agile,\t1\n",
      "all\t5\n",
      "alleviated\t2\n",
      "allow\t1\n",
      "allowing\t1\n",
      "almost\t1\n",
      "also\t7\n",
      "alternative\t1\n",
      "among\t1\n",
      "amount\t1\n",
      "amounts\t2\n",
      "an\t4\n",
      "analysis\t9\n",
      "analysis.\t1\n",
      "analysis;\t1\n",
      "analyst\t1\n",
      "analytical\t3\n",
      "analytics\t27\n",
      "analytics,\t4\n",
      "analyze\t2\n",
      "analyzing\t1\n",
      "and\t178\n",
      "anomalies\t1\n",
      "application\t2\n",
      "applications\t2\n",
      "applications,\t1\n",
      "applications.\t1\n",
      "applied\t1\n",
      "applies\t1\n",
      "apply\t2\n",
      "approach\t7\n",
      "approach.\t2\n",
      "approaches\t12\n",
      "approaches.\t2\n",
      "appropriately\t1\n",
      "apps\t1\n",
      "architecture\t1\n",
      "architectures\t1\n",
      "archiving.\t1\n",
      "are\t27\n",
      "area\t2\n",
      "areas\t3\n",
      "around\t1\n",
      "as\t38\n",
      "ask\t1\n",
      "aspects\t2\n",
      "assistants\t1\n",
      "assistants.\t1\n",
      "at\t5\n",
      "attain\t1\n",
      "audio,\t2\n",
      "augmented\t1\n",
      "automated\t2\n",
      "automatically\t1\n",
      "automation;\t1\n",
      "autonomous\t2\n",
      "availability\t3\n",
      "available\t1\n",
      "available,\t1\n",
      "awareness\t3\n",
      "awareness.\t1\n",
      "away\t1\n",
      "back\t1\n",
      "bandwidth\t1\n",
      "banking\t2\n",
      "barrier\t2\n",
      "be\t8\n",
      "because\t2\n",
      "become\t2\n",
      "becoming\t2\n",
      "been\t1\n",
      "before\t1\n",
      "behavior,\t1\n",
      "behavioral\t1\n",
      "behind\t1\n",
      "being\t2\n",
      "benefit\t1\n",
      "benefits\t2\n",
      "benefits,\t1\n",
      "better\t5\n",
      "between\t2\n",
      "beyond\t2\n",
      "bias\t1\n",
      "big\t28\n",
      "biggest\t1\n",
      "both\t2\n",
      "boundaries.\t1\n",
      "breaches,\t1\n",
      "breakthroughs\t1\n",
      "briefings\t1\n",
      "brings\t1\n",
      "brokers,\t2\n",
      "business\t13\n",
      "but\t6\n",
      "by\t16\n",
      "by:\t1\n",
      "can\t10\n",
      "capabilities\t1\n",
      "capabilities.\t1\n",
      "capability\t1\n",
      "capable\t1\n",
      "casual\t1\n",
      "cause\t1\n",
      "caused\t1\n",
      "centers\t1\n",
      "centers.\t1\n",
      "central\t1\n",
      "centralize\t1\n",
      "certain\t2\n",
      "challenge\t1\n",
      "challenged\t2\n",
      "challenges\t5\n",
      "challenges.\t1\n",
      "changed\t2\n",
      "changes\t2\n",
      "changing\t1\n",
      "channels.\t1\n",
      "charts,\t1\n",
      "chatbots\t2\n",
      "check\t1\n",
      "circle\t1\n",
      "circles,\t1\n",
      "classification\t1\n",
      "cloud\t15\n",
      "cloud-based\t1\n",
      "collaborative\t2\n",
      "collect\t1\n",
      "collecting\t2\n",
      "collecting,\t1\n",
      "combination\t1\n",
      "combine\t1\n",
      "come.\t1\n",
      "comes\t3\n",
      "coming\t1\n",
      "common\t1\n",
      "companies\t4\n",
      "compelled\t1\n",
      "compelling\t1\n",
      "complete\t2\n",
      "complex\t5\n",
      "compute\t1\n",
      "computing\t7\n",
      "computing,\t1\n",
      "computing.\t1\n",
      "concept\t2\n",
      "conceptual\t1\n",
      "concerns\t1\n",
      "conditions\t1\n",
      "conducted\t1\n",
      "consider\t2\n",
      "constantly\t1\n",
      "contains\t2\n",
      "context\t6\n",
      "context-relevant\t1\n",
      "continue\t2\n",
      "continued\t1\n",
      "continues\t1\n",
      "conventional\t1\n",
      "correlations\t1\n",
      "costs,\t2\n",
      "created\t1\n",
      "creating\t1\n",
      "critical\t5\n",
      "crosses\t1\n",
      "curse\t1\n",
      "customer\t6\n",
      "customers\t1\n",
      "customers.\t3\n",
      "dark\t1\n",
      "data\t135\n",
      "data)\t1\n",
      "data).\t1\n",
      "data,\t8\n",
      "data-hungry\t6\n",
      "data-sharing\t1\n",
      "data.\t9\n",
      "data:\t1\n",
      "data;\t2\n",
      "databases\t2\n",
      "datasets\t1\n",
      "data\t7\n",
      "deal\t2\n",
      "dealing\t4\n",
      "decade,\t1\n",
      "decades,\t1\n",
      "decision\t6\n",
      "decision-making.\t1\n",
      "deep\t4\n",
      "deeper\t1\n",
      "deliver\t1\n",
      "demand\t2\n",
      "demanding\t2\n",
      "demanding,\t1\n",
      "den\t2\n",
      "dependency\t2\n",
      "dependent\t1\n",
      "deployments.\t1\n",
      "deposit\t1\n",
      "derive\t1\n",
      "designed\t1\n",
      "detect\t1\n",
      "detection\t1\n",
      "determining\t1\n",
      "developed\t1\n",
      "developing\t1\n",
      "development\t1\n",
      "devices\t6\n",
      "devices,\t1\n",
      "diagram\t1\n",
      "different\t2\n",
      "digital\t2\n",
      "digitized.\t1\n",
      "dimensionality)\t1\n",
      "disruptions\t1\n",
      "disruptions,\t2\n",
      "distributed\t2\n",
      "diverse\t4\n",
      "diversity\t4\n",
      "division\t1\n",
      "documents,\t1\n",
      "dominant\t1\n",
      "doubt\t2\n",
      "dramatically\t1\n",
      "drive\t2\n",
      "driven\t1\n",
      "driving\t3\n",
      "due\t1\n",
      "dynamic.\t1\n",
      "dynamics\t2\n",
      "easily\t2\n",
      "economic\t1\n",
      "edge\t2\n",
      "effectively,\t1\n",
      "efficiency\t1\n",
      "either\t1\n",
      "embodied\t1\n",
      "emerge.\t1\n",
      "emergence\t1\n",
      "emerging\t2\n",
      "emerging.\t2\n",
      "emotional\t1\n",
      "empowers\t1\n",
      "enable\t4\n",
      "enabler\t3\n",
      "enables\t4\n",
      "enabling\t3\n",
      "end\t1\n",
      "energy\t2\n",
      "enrich\t1\n",
      "enrichment\t1\n",
      "enterprise\t1\n",
      "enterprises\t4\n",
      "entry\t1\n",
      "entry.\t1\n",
      "environments\t2\n",
      "environments.\t1\n",
      "envisage\t1\n",
      "era,\t1\n",
      "eroding\t1\n",
      "especially\t4\n",
      "even\t6\n",
      "events\t1\n",
      "evolution\t2\n",
      "evolving\t1\n",
      "evolving.\t1\n",
      "exacerbated\t1\n",
      "example,\t3\n",
      "examples\t1\n",
      "exchanges\t1\n",
      "exciting\t1\n",
      "expanding\t1\n",
      "expected\t1\n",
      "expenses.\t1\n",
      "experience\t4\n",
      "expertise.\t1\n",
      "experts\t1\n",
      "explains\t1\n",
      "explanatory\t1\n",
      "explosion\t1\n",
      "extend\t1\n",
      "external\t4\n",
      "extract,\t1\n",
      "extracting\t1\n",
      "familiar\t1\n",
      "far\t1\n",
      "fashion\t1\n",
      "faster\t1\n",
      "faster,\t1\n",
      "features\t1\n",
      "federated,\t2\n",
      "fell\t1\n",
      "few-shot\t2\n",
      "files,\t1\n",
      "finance,\t1\n",
      "financial\t1\n",
      "find\t2\n",
      "finding\t3\n",
      "flow\t1\n",
      "flows\t1\n",
      "focus\t1\n",
      "focused\t1\n",
      "focuses\t1\n",
      "following:\t1\n",
      "for\t47\n",
      "forcing\t1\n",
      "fore\t1\n",
      "fore,\t1\n",
      "forecasting\t1\n",
      "foreseeable\t1\n",
      "form,\t1\n",
      "format.\t1\n",
      "formats\t2\n",
      "formats.\t2\n",
      "forms\t2\n",
      "forth\t1\n",
      "found\t1\n",
      "foundational\t1\n",
      "four\t1\n",
      "frameworks\t1\n",
      "fraud\t1\n",
      "from\t15\n",
      "full\t1\n",
      "further\t3\n",
      "future.\t1\n",
      "gamut\t1\n",
      "gather\t1\n",
      "generated\t2\n",
      "generated,\t1\n",
      "generation\t2\n",
      "generation,\t2\n",
      "generators\t1\n",
      "get\t1\n",
      "governance,\t2\n",
      "government\t1\n",
      "graph\t2\n",
      "graphs\t2\n",
      "greater\t4\n",
      "growing\t3\n",
      "growth\t2\n",
      "had\t1\n",
      "hand\t1\n",
      "handle\t3\n",
      "handled\t1\n",
      "hands\t1\n",
      "happen\t1\n",
      "happens\t1\n",
      "harder\t1\n",
      "has\t3\n",
      "have\t5\n",
      "having\t2\n",
      "healthcare\t2\n",
      "healthcare,\t2\n",
      "heavily\t1\n",
      "help\t1\n",
      "helping\t2\n",
      "helps\t2\n",
      "historical\t2\n",
      "how\t2\n",
      "https://www.techtarget.com/searchdatamanagement/feature/Top-trends-in-big-data-for-2021-and-beyond\t1\n",
      "human\t1\n",
      "humans\t1\n",
      "hungry.\t1\n",
      "hybrid\t4\n",
      "hyperpersonalization\t1\n",
      "icons\t1\n",
      "identify\t1\n",
      "if\t1\n",
      "image,\t3\n",
      "images\t1\n",
      "images,\t1\n",
      "implement\t1\n",
      "improve\t5\n",
      "improved\t1\n",
      "improvement.\t1\n",
      "in\t60\n",
      "include\t9\n",
      "include,\t1\n",
      "includes\t2\n",
      "including\t2\n",
      "incorporating\t1\n",
      "increase\t1\n",
      "increased\t2\n",
      "increases\t2\n",
      "increasing\t2\n",
      "increasingly\t4\n",
      "industries\t3\n",
      "industries.\t1\n",
      "industry\t1\n",
      "inexorable\t1\n",
      "information\t6\n",
      "information.\t2\n",
      "infrastructure\t3\n",
      "infrastructure,\t2\n",
      "infrastructure.\t2\n",
      "infrastructures\t1\n",
      "initiatives\t2\n",
      "initiatives,\t1\n",
      "innovation\t3\n",
      "innovation)\t1\n",
      "innovation.\t1\n",
      "innovations\t3\n",
      "inquiries\t1\n",
      "insights\t1\n",
      "insights.\t2\n",
      "instead\t1\n",
      "instead,\t1\n",
      "insurance,\t1\n",
      "intelligence\t2\n",
      "intelligent\t1\n",
      "intelligent,\t1\n",
      "intentions\t1\n",
      "interactions\t1\n",
      "internal\t2\n",
      "international\t1\n",
      "into\t4\n",
      "investing\t1\n",
      "is\t28\n",
      "isn't\t1\n",
      "issues,\t1\n",
      "it\t7\n",
      "it's\t1\n",
      "it.\t1\n",
      "iterative\t1\n",
      "its\t3\n",
      "itself\t2\n",
      "just\t2\n",
      "key\t1\n",
      "knowledge\t1\n",
      "known\t3\n",
      "labeled\t1\n",
      "lack\t2\n",
      "lake\t2\n",
      "lake.\t1\n",
      "lakes\t2\n",
      "language\t1\n",
      "language,\t1\n",
      "large\t6\n",
      "large,\t3\n",
      "largely\t1\n",
      "lax\t1\n",
      "leaders\t3\n",
      "learn\t1\n",
      "learning\t9\n",
      "learning,\t3\n",
      "learning.\t4\n",
      "leaves\t1\n",
      "led\t1\n",
      "left\t1\n",
      "legacy\t1\n",
      "less\t6\n",
      "let\t1\n",
      "levels\t1\n",
      "leverage\t1\n",
      "leveraging\t2\n",
      "liable\t1\n",
      "lifecycle\t2\n",
      "lifecycle,\t1\n",
      "lifecycle.\t1\n",
      "like\t1\n",
      "limitations\t3\n",
      "limitations.\t1\n",
      "limited\t1\n",
      "limitless\t1\n",
      "links\t2\n",
      "load\t2\n",
      "logs,\t1\n",
      "look\t1\n",
      "low\t1\n",
      "lower\t2\n",
      "lowers\t1\n",
      "machine\t5\n",
      "main\t1\n",
      "maintain\t1\n",
      "major\t1\n",
      "make\t4\n",
      "makers\t1\n",
      "making\t4\n",
      "making,\t2\n",
      "manage\t2\n",
      "manage,\t1\n",
      "management\t2\n",
      "management,\t2\n",
      "managing\t2\n",
      "manner.\t1\n",
      "manufacturing\t1\n",
      "many\t4\n",
      "market\t3\n",
      "marketplaces\t2\n",
      "marketplaces.\t2\n",
      "markets.\t1\n",
      "massive\t1\n",
      "mean\t1\n",
      "meaning\t1\n",
      "means\t2\n",
      "media,\t2\n",
      "meet\t2\n",
      "methodology\t1\n",
      "min\t1\n",
      "mobile\t1\n",
      "modeling\t1\n",
      "models\t1\n",
      "models,\t2\n",
      "models.\t1\n",
      "modern\t1\n",
      "monolithic\t1\n",
      "more\t29\n",
      "mostly\t1\n",
      "motion,\t1\n",
      "move\t3\n",
      "moving\t3\n",
      "much\t1\n",
      "must\t1\n",
      "native\t1\n",
      "natural\t2\n",
      "need\t8\n",
      "needs\t5\n",
      "needs,\t1\n",
      "needs.\t2\n",
      "network,\t1\n",
      "networks.\t1\n",
      "new\t6\n",
      "next-generation\t1\n",
      "no\t2\n",
      "not\t2\n",
      "number\t2\n",
      "obsolete\t1\n",
      "of\t108\n",
      "offer\t2\n",
      "often\t2\n",
      "on\t16\n",
      "on-premises\t1\n",
      "one\t2\n",
      "one-size-fits-all\t2\n",
      "ones\t1\n",
      "ones.\t1\n",
      "open\t1\n",
      "operate.\t1\n",
      "operations\t1\n",
      "operations.\t1\n",
      "optimal\t1\n",
      "optimization\t1\n",
      "optimize\t2\n",
      "optimized\t1\n",
      "optimizes\t1\n",
      "or\t15\n",
      "organization\t3\n",
      "organization.\t1\n",
      "organizational\t1\n",
      "organizations\t23\n",
      "organizations,\t2\n",
      "organizations.\t1\n",
      "organizations\t2\n",
      "other\t5\n",
      "outcomes.\t1\n",
      "over\t3\n",
      "overall\t1\n",
      "overlapping\t1\n",
      "overly\t2\n",
      "own\t3\n",
      "pace\t1\n",
      "paired\t1\n",
      "pandemic\t1\n",
      "paper\t1\n",
      "particular,\t1\n",
      "partly\t1\n",
      "past\t4\n",
      "past,\t1\n",
      "patient\t2\n",
      "patterns\t2\n",
      "pay\t1\n",
      "people\t1\n",
      "perceived\t2\n",
      "performance\t1\n",
      "personal\t1\n",
      "personalization\t1\n",
      "personalized\t1\n",
      "petabytes\t1\n",
      "physical\t1\n",
      "piecemeal\t1\n",
      "place\t1\n",
      "plans\t1\n",
      "platforms\t1\n",
      "platforms,\t1\n",
      "plots.\t1\n",
      "power\t4\n",
      "practice\t1\n",
      "practices\t2\n",
      "practices,\t1\n",
      "predictive\t3\n",
      "preparation\t1\n",
      "prevent\t1\n",
      "previous\t1\n",
      "priorities\t1\n",
      "privacy\t3\n",
      "problems\t1\n",
      "process\t2\n",
      "process,\t1\n",
      "processes\t2\n",
      "processes,\t1\n",
      "processes.\t1\n",
      "processing\t13\n",
      "processing,\t3\n",
      "processing.\t3\n",
      "products\t1\n",
      "progress\t1\n",
      "prohibitive.\t1\n",
      "promise\t1\n",
      "promoted\t1\n",
      "properly\t1\n",
      "protection\t1\n",
      "provide\t5\n",
      "providers\t3\n",
      "provides\t2\n",
      "providing\t1\n",
      "proving\t1\n",
      "public\t2\n",
      "public-sector\t1\n",
      "published\t1\n",
      "putting\t1\n",
      "quality\t1\n",
      "quality,\t1\n",
      "quantities\t1\n",
      "query\t1\n",
      "questions\t1\n",
      "quickly.\t1\n",
      "ramp-up\t1\n",
      "range\t7\n",
      "rapid\t3\n",
      "rapidly\t2\n",
      "rate\t1\n",
      "rather\t3\n",
      "read\t1\n",
      "reading),\t1\n",
      "real\t3\n",
      "realizing\t1\n",
      "recent\t1\n",
      "recognition\t1\n",
      "recommendation\t1\n",
      "recommended\t1\n",
      "reconsider,\t1\n",
      "reducing\t4\n",
      "reexamine\t1\n",
      "reflect\t1\n",
      "regulated\t1\n",
      "regulations\t1\n",
      "regulatory\t1\n",
      "regulatory-friendly\t1\n",
      "reinforcement,\t2\n",
      "relate\t1\n",
      "related\t3\n",
      "relevant\t1\n",
      "reliance\t1\n",
      "relying\t2\n",
      "remote\t1\n",
      "reporting\t1\n",
      "represented\t1\n",
      "require\t2\n",
      "required\t1\n",
      "requires\t1\n",
      "requiring\t2\n",
      "research\t1\n",
      "residing\t1\n",
      "resources\t1\n",
      "respondents\t1\n",
      "responses\t1\n",
      "responsibility\t2\n",
      "responsible\t2\n",
      "responsive\t1\n",
      "rest\t1\n",
      "restrictions\t1\n",
      "result\t1\n",
      "result,\t1\n",
      "resulting\t1\n",
      "results\t2\n",
      "retail,\t2\n",
      "revolutionary\t1\n",
      "richer\t2\n",
      "richer,\t2\n",
      "right\t2\n",
      "rise\t1\n",
      "robots,\t1\n",
      "robust\t3\n",
      "robustness,\t1\n",
      "said\t1\n",
      "scale.\t1\n",
      "science\t1\n",
      "sea\t1\n",
      "sector,\t1\n",
      "secure\t2\n",
      "secured\t1\n",
      "security\t3\n",
      "see\t1\n",
      "seeing\t1\n",
      "seek\t1\n",
      "seeking.\t1\n",
      "self-supervised\t2\n",
      "semistructured\t2\n",
      "send\t1\n",
      "sensors,\t3\n",
      "sensory\t1\n",
      "sent\t1\n",
      "separate\t1\n",
      "servers.\t1\n",
      "service\t1\n",
      "services\t3\n",
      "services;\t1\n",
      "sets\t2\n",
      "shared\t1\n",
      "sharing\t2\n",
      "sharing,\t1\n",
      "shied\t1\n",
      "shift\t1\n",
      "shifting\t2\n",
      "shifts\t2\n",
      "short\t1\n",
      "should\t4\n",
      "should:\t1\n",
      "showing\t1\n",
      "shows\t1\n",
      "side\t1\n",
      "sign\t1\n",
      "significant\t2\n",
      "single\t1\n",
      "sitting\t1\n",
      "situation\t1\n",
      "situational\t4\n",
      "sizes\t2\n",
      "slow\t1\n",
      "small\t12\n",
      "small,\t1\n",
      "smart\t1\n",
      "smarter,\t1\n",
      "smartphones\t1\n",
      "smell\t2\n",
      "so\t1\n",
      "so-called\t1\n",
      "social\t2\n",
      "solutions\t1\n",
      "somewhat\t1\n",
      "source\t1\n",
      "sources\t6\n",
      "sources,\t5\n",
      "sources.\t5\n",
      "sourcing,\t1\n",
      "space\t2\n",
      "speed\t1\n",
      "speed.\t1\n",
      "spend\t1\n",
      "spending\t3\n",
      "spot\t1\n",
      "spur\t1\n",
      "spurring\t1\n",
      "staff.\t1\n",
      "standing\t2\n",
      "stay,\t1\n",
      "stays\t1\n",
      "stewardship\t2\n",
      "still\t3\n",
      "storage\t9\n",
      "storage,\t2\n",
      "store\t3\n",
      "stored\t1\n",
      "stores\t1\n",
      "storing\t2\n",
      "strategies\t1\n",
      "strategy\t2\n",
      "streaming\t1\n",
      "structured\t5\n",
      "structured,\t1\n",
      "such\t15\n",
      "sufficiently\t1\n",
      "supervised\t1\n",
      "support\t4\n",
      "sure\t1\n",
      "survey\t1\n",
      "survey,\t1\n",
      "synergy\t2\n",
      "synthetic\t3\n",
      "system\t2\n",
      "systems\t7\n",
      "systems,\t2\n",
      "systems.\t2\n",
      "table\t1\n",
      "tabular,\t2\n",
      "tackle\t1\n",
      "tailored\t2\n",
      "tangible\t1\n",
      "tasks\t1\n",
      "taxing\t1\n",
      "teams\t2\n",
      "technical\t2\n",
      "techniques\t13\n",
      "techniques,\t2\n",
      "technologies\t2\n",
      "technologies,\t2\n",
      "technologies.\t1\n",
      "technology\t5\n",
      "telemedicine\t1\n",
      "temperature\t1\n",
      "temperature,\t1\n",
      "text\t3\n",
      "text,\t2\n",
      "text.\t1\n",
      "than\t6\n",
      "that\t29\n",
      "the\t89\n",
      "their\t20\n",
      "them\t3\n",
      "themselves\t1\n",
      "themselves,\t1\n",
      "then\t1\n",
      "there\t1\n",
      "these\t3\n",
      "they\t2\n",
      "they're\t2\n",
      "think\t2\n",
      "thinking\t1\n",
      "third-party\t1\n",
      "this\t7\n",
      "those\t3\n",
      "three\t1\n",
      "through\t9\n",
      "throughout\t1\n",
      "time\t2\n",
      "time-intensive\t1\n",
      "time-series\t2\n",
      "time.\t1\n",
      "to\t88\n",
      "to,\t1\n",
      "together,\t1\n",
      "toolbox\t2\n",
      "tools\t4\n",
      "top\t1\n",
      "toward\t1\n",
      "tracked\t1\n",
      "traditional\t2\n",
      "training\t1\n",
      "transactions\t1\n",
      "transfer\t2\n",
      "transform\t1\n",
      "transformation\t2\n",
      "transportation,\t1\n",
      "trend\t2\n",
      "trends\t5\n",
      "trends.\t1\n",
      "trust\t1\n",
      "trying\t1\n",
      "turn\t1\n",
      "turn,\t2\n",
      "turning\t1\n",
      "twins.\t2\n",
      "two\t1\n",
      "types\t2\n",
      "typically\t1\n",
      "understand\t1\n",
      "unprocessed\t1\n",
      "unstructured\t6\n",
      "unstructured,\t2\n",
      "unused\t1\n",
      "up\t1\n",
      "us\t1\n",
      "use\t8\n",
      "used\t2\n",
      "useful\t2\n",
      "user.\t1\n",
      "users\t2\n",
      "users,\t1\n",
      "users.\t1\n",
      "uses\t1\n",
      "using\t5\n",
      "usually\t1\n",
      "utilizing\t1\n",
      "value\t3\n",
      "variety\t7\n",
      "variety,\t1\n",
      "varying\t1\n",
      "vast\t3\n",
      "vendor\t1\n",
      "veracity\t1\n",
      "via\t1\n",
      "vibration.\t2\n",
      "video\t2\n",
      "video,\t2\n",
      "videos,\t1\n",
      "view.\t2\n",
      "visibility\t2\n",
      "visualization\t2\n",
      "visualization.\t1\n",
      "visualized\t1\n",
      "voice\t2\n",
      "voice,\t2\n",
      "volume\t2\n",
      "volumes\t2\n",
      "voluminous\t1\n",
      "waiting\t1\n",
      "warehouse\t3\n",
      "was\t1\n",
      "way\t1\n",
      "ways\t1\n",
      "we\t1\n",
      "wearables\t1\n",
      "web\t1\n",
      "websites\t1\n",
      "well\t3\n",
      "were\t1\n",
      "what\t2\n",
      "when\t2\n",
      "where\t1\n",
      "which\t3\n",
      "who\t1\n",
      "wide\t18\n",
      "wider\t1\n",
      "widespread\t1\n",
      "will\t5\n",
      "with\t21\n",
      "without\t4\n",
      "work\t1\n",
      "working\t2\n",
      "years\t1\n",
      "\t2\n",
      "big\t1\n",
      "small\t3\n",
      "wide\t3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# print the output of wordcount\n",
    "echo -e \"\\nwordcount output:\"\n",
    "hdfs dfs -cat ${work_dir}/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. WordCount with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next example will use [Hadoop streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Python scripts are used `mapper.py` and `reducer.py`, let's look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** MAPPER.PY ****************\n",
      "\n",
      "#! /usr/bin/python\n",
      "\n",
      "import sys\n",
      "\n",
      "def do_map(doc): \n",
      "    for word in doc.split(): \n",
      "        yield word.lower(), 1 \n",
      "\n",
      "for line in sys.stdin: \n",
      "    for key, value in do_map(line): \n",
      "        print(key + '\\t' + str(value))\n",
      "\n",
      "************** REDUCER.PY ****************\n",
      "\n",
      "#! /usr/bin/python\n",
      "\n",
      "import sys\n",
      " \n",
      "def do_reduce(key, values):\n",
      "    return key, sum(values)\n",
      "\n",
      "prev_key = None\n",
      "values = []\n",
      "\n",
      "for line in sys.stdin:\n",
      "    key, value = line.split('\\t')\n",
      "    if key != prev_key and prev_key is not None:\n",
      "        result_key, result_value = do_reduce(prev_key, values)\n",
      "        print(result_key + '\\t' + str(result_value))\n",
      "        values = []\n",
      "    prev_key = key\n",
      "    values.append(int(value))\n",
      "\n",
      "if prev_key is not None:\n",
      "    result_key, result_value = do_reduce(prev_key, values)\n",
      "    print(result_key + '\\t' + str(result_value))\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo -e \"\\n************** MAPPER.PY ****************\\n\"\n",
    "cat ./utils/mapper.py\n",
    "echo -e \"\\n************** REDUCER.PY ****************\\n\"\n",
    "cat ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How this Python code works out of Hadoop\n",
    "\n",
    "First of all, a few words about bash `stdin` and `stdout`. Here is a [good article](https://medium.com/linuxstories/bash-pipes-and-redirections-4c267c13643b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a file.\n",
      "I am in the HDFS now and feel good."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat test_hdfs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,\t1\n",
      "i\t1\n",
      "am\t1\n",
      "a\t1\n",
      "file.\t1\n",
      "i\t1\n",
      "am\t1\n",
      "in\t1\n",
      "the\t1\n",
      "hdfs\t1\n",
      "now\t1\n",
      "and\t1\n",
      "feel\t1\n",
      "good.\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# let's send our file to `stdin` of our mapper\n",
    "# `cat` is to list content of the file\n",
    "# pipe `|` is for sending that output to our `mapper.py` as input\n",
    "\n",
    "cat test_hdfs.txt | python ./utils/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# write result of mapper to the file\n",
    "\n",
    "cat test_hdfs.txt | python ./utils/mapper.py > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,\t1\n",
      "i\t1\n",
      "am\t1\n",
      "a\t1\n",
      "file.\t1\n",
      "i\t1\n",
      "am\t1\n",
      "in\t1\n",
      "the\t1\n",
      "hdfs\t1\n",
      "now\t1\n",
      "and\t1\n",
      "feel\t1\n",
      "good.\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "am\t2\n",
      "and\t1\n",
      "feel\t1\n",
      "file.\t1\n",
      "good.\t1\n",
      "hdfs\t1\n",
      "hello,\t1\n",
      "i\t2\n",
      "in\t1\n",
      "now\t1\n",
      "the\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat result.txt | sort -t 1 | python ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "am\t2\n",
      "and\t1\n",
      "feel\t1\n",
      "file.\t1\n",
      "good.\t1\n",
      "hdfs\t1\n",
      "hello,\t1\n",
      "i\t2\n",
      "in\t1\n",
      "now\t1\n",
      "the\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat test_hdfs.txt | python ./utils/mapper.py | sort -t 1 | python ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python code within Hadoop (YARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our Python MapReduce scripts in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan hadoopusers       9161 2023-05-02 15:31 /jovyan/input/big_data_trends_2021.txt\r\n",
      "-rw-r--r--   1 jovyan hadoopusers      11836 2023-05-02 15:31 /jovyan/input/big_data_trends_2023.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the job and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./utils/mapper.py, ./utils/reducer.py, /tmp/hadoop-unjar5713892345926893084/] [] /tmp/streamjob8458008549747069715.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `/jovyan/output_py': No such file or directory\n",
      "2023-05-02 15:31:55,927 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "2023-05-02 15:31:56,645 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-05-02 15:31:56,803 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-05-02 15:31:56,983 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1683041087371_0002\n",
      "2023-05-02 15:31:58,060 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2023-05-02 15:31:58,916 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2023-05-02 15:31:59,409 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1683041087371_0002\n",
      "2023-05-02 15:31:59,411 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-05-02 15:31:59,546 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-05-02 15:31:59,546 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-05-02 15:31:59,599 INFO impl.YarnClientImpl: Submitted application application_1683041087371_0002\n",
      "2023-05-02 15:31:59,632 INFO mapreduce.Job: The url to track the job: http://0.0.0.0:60000/proxy/application_1683041087371_0002/\n",
      "2023-05-02 15:31:59,634 INFO mapreduce.Job: Running job: job_1683041087371_0002\n",
      "2023-05-02 15:32:04,703 INFO mapreduce.Job: Job job_1683041087371_0002 running in uber mode : false\n",
      "2023-05-02 15:32:04,704 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-05-02 15:32:09,757 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-05-02 15:32:14,782 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-05-02 15:32:15,792 INFO mapreduce.Job: Job job_1683041087371_0002 completed successfully\n",
      "2023-05-02 15:32:15,869 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=33594\n",
      "\t\tFILE: Number of bytes written=1021929\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=22647\n",
      "\t\tHDFS: Number of bytes written=11244\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9219\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1777\n",
      "\t\tTotal time spent by all map tasks (ms)=9219\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1777\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9219\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1777\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9440256\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1819648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=117\n",
      "\t\tMap output records=3155\n",
      "\t\tMap output bytes=27278\n",
      "\t\tMap output materialized bytes=33606\n",
      "\t\tInput split bytes=312\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1074\n",
      "\t\tReduce shuffle bytes=33606\n",
      "\t\tReduce input records=3155\n",
      "\t\tReduce output records=1074\n",
      "\t\tSpilled Records=6310\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=329\n",
      "\t\tCPU time spent (ms)=2090\n",
      "\t\tPhysical memory (bytes) snapshot=1108672512\n",
      "\t\tVirtual memory (bytes) snapshot=10185060352\n",
      "\t\tTotal committed heap usage (bytes)=1071120384\n",
      "\t\tPeak Map Physical memory (bytes)=323342336\n",
      "\t\tPeak Map Virtual memory (bytes)=2546089984\n",
      "\t\tPeak Reduce Physical memory (bytes)=225222656\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2551595008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=22335\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=11244\n",
      "2023-05-02 15:32:15,869 INFO streaming.StreamJob: Output directory: /jovyan/output_py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_py\n",
    "\n",
    "# delete directory if exists\n",
    "hdfs dfs -rm -r ${work_dir}${out_dir}\n",
    "\n",
    "yarn jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.2.jar \\\n",
    "    -input ${work_dir}/input/*.txt -output ${work_dir}${out_dir} \\\n",
    "    -file ./utils/mapper.py -file ./utils/reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" -reducer \"python3 reducer.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for Hadoop streaming:\n",
    "\n",
    "| Option | Description| \n",
    "| --- | --- |\n",
    "| -files| A command-separated list of files to be copied to the MapReduce cluster |\n",
    "| -mapper | The command to be run as the mapper |\n",
    "| -reducer | The command to be run as the reducer |\n",
    "| -input | The DFS input path for the Map step |\n",
    "| -output | The DFS output directory for the Reduce step |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 jovyan hadoopusers          0 2023-05-02 15:32 /jovyan/output_py/_SUCCESS\n",
      "-rw-r--r--   1 jovyan hadoopusers      11244 2023-05-02 15:32 /jovyan/output_py/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_py\n",
    "\n",
    "hdfs dfs -ls ${work_dir}/${out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(09\t1\n",
      "(balanced\t1\n",
      "(context,\t1\n",
      "(d&a)\t1\n",
      "(iot)\t1\n",
      "(ml).\t1\n",
      "(real-time)\t1\n",
      "(see\t2\n",
      "(small\t1\n",
      "(velocity)\t1\n",
      "(veracity),\t1\n",
      "(wide\t1\n",
      "-\t2\n",
      "--\t8\n",
      "05\t1\n",
      "1).\t1\n",
      "1.\t2\n",
      "10\t2\n",
      "16\t1\n",
      "193\t1\n",
      "2.\t1\n",
      "2020,\t1\n",
      "2020.\t1\n",
      "2021\t1\n",
      "2021)\t1\n",
      "2021:\t2\n",
      "2023\t4\n",
      "2023.\t1\n",
      "2025,\t1\n",
      "3.\t1\n",
      "360-degree\t2\n",
      "4\t1\n",
      "4.\t1\n",
      "6\t1\n",
      "63%\t1\n",
      "70%\t1\n",
      "a\t46\n",
      "ability\t1\n",
      "able\t2\n",
      "about\t6\n",
      "accelerate.\t1\n",
      "accuracy\t1\n",
      "accurate\t2\n",
      "achieved\t1\n",
      "across\t5\n",
      "actions:\t1\n",
      "adaptive\t1\n",
      "adaptive,\t2\n",
      "addition\t1\n",
      "addition,\t3\n",
      "additional\t1\n",
      "address\t1\n",
      "address.\t1\n",
      "adoption\t1\n",
      "advanced\t7\n",
      "advances\t1\n",
      "advancing\t1\n",
      "advantages\t1\n",
      "aggregate\t1\n",
      "agile,\t1\n",
      "ai\t13\n",
      "ai,\t5\n",
      "ai-enabled\t2\n",
      "ai.\t1\n",
      "all\t5\n",
      "alleviated\t2\n",
      "allow\t1\n",
      "allowing\t1\n",
      "almost\t1\n",
      "also\t7\n",
      "alternative\t1\n",
      "among\t1\n",
      "amount\t1\n",
      "amounts\t2\n",
      "an\t4\n",
      "analysis\t10\n",
      "analysis.\t1\n",
      "analysis;\t1\n",
      "analyst\t1\n",
      "analytical\t3\n",
      "analytics\t33\n",
      "analytics,\t5\n",
      "analyze\t2\n",
      "analyzing\t1\n",
      "and\t178\n",
      "android\t1\n",
      "anomalies\t1\n",
      "apple\t1\n",
      "application\t2\n",
      "applications\t2\n",
      "applications,\t1\n",
      "applications.\t1\n",
      "applied\t1\n",
      "applies\t1\n",
      "apply\t2\n",
      "approach\t7\n",
      "approach.\t2\n",
      "approaches\t12\n",
      "approaches.\t2\n",
      "appropriately\t1\n",
      "apps\t1\n",
      "architecture\t1\n",
      "architectures\t1\n",
      "archiving.\t1\n",
      "are\t27\n",
      "area\t2\n",
      "areas\t3\n",
      "around\t1\n",
      "as\t42\n",
      "ask\t1\n",
      "aspects\t2\n",
      "assistants\t1\n",
      "assistants.\t1\n",
      "assumption\t1\n",
      "at\t5\n",
      "attain\t1\n",
      "audio,\t2\n",
      "augmented\t1\n",
      "automated\t2\n",
      "automatically\t1\n",
      "automation;\t1\n",
      "autonomous\t2\n",
      "availability\t3\n",
      "available\t1\n",
      "available,\t1\n",
      "awareness\t3\n",
      "awareness.\t1\n",
      "away\t1\n",
      "aws,\t1\n",
      "back\t1\n",
      "bandwidth\t1\n",
      "banking\t2\n",
      "barrier\t2\n",
      "be\t8\n",
      "because\t2\n",
      "become\t2\n",
      "becoming\t2\n",
      "been\t1\n",
      "before\t1\n",
      "behavior,\t1\n",
      "behavioral\t1\n",
      "behind\t1\n",
      "being\t2\n",
      "benefit\t1\n",
      "benefits\t2\n",
      "benefits,\t1\n",
      "better\t5\n",
      "between\t2\n",
      "beyond\t2\n",
      "bias\t1\n",
      "big\t35\n",
      "biggest\t1\n",
      "both\t2\n",
      "boundaries.\t1\n",
      "breaches,\t1\n",
      "breakthroughs\t1\n",
      "briefings\t1\n",
      "brings\t1\n",
      "brokers,\t2\n",
      "business\t13\n",
      "but\t7\n",
      "by\t20\n",
      "by:\t1\n",
      "can\t10\n",
      "capabilities\t1\n",
      "capabilities.\t1\n",
      "capability\t1\n",
      "capable\t1\n",
      "casual\t1\n",
      "cause\t1\n",
      "caused\t1\n",
      "centers\t1\n",
      "centers.\t1\n",
      "central\t1\n",
      "centralize\t1\n",
      "certain\t2\n",
      "challenge\t1\n",
      "challenged\t2\n",
      "challenges\t5\n",
      "challenges.\t1\n",
      "changed\t2\n",
      "changes\t3\n",
      "changing\t1\n",
      "channels.\t1\n",
      "charts,\t1\n",
      "chatbots\t2\n",
      "check\t1\n",
      "choudhary,\t1\n",
      "circle\t1\n",
      "circles,\t1\n",
      "classification\t1\n",
      "cloud\t15\n",
      "cloud-based\t1\n",
      "clougherty\t1\n",
      "cognilytica\t1\n",
      "collaborative\t2\n",
      "collect\t1\n",
      "collecting\t2\n",
      "collecting,\t1\n",
      "collectively,\t1\n",
      "combination\t1\n",
      "combine\t1\n",
      "come.\t1\n",
      "comes\t3\n",
      "coming\t1\n",
      "common\t1\n",
      "companies\t4\n",
      "compelled\t1\n",
      "compelling\t1\n",
      "complete\t2\n",
      "complex\t5\n",
      "compute\t1\n",
      "computing\t7\n",
      "computing,\t1\n",
      "computing.\t1\n",
      "concept\t2\n",
      "conceptual\t1\n",
      "concerns\t1\n",
      "conditions\t1\n",
      "conducted\t1\n",
      "consider\t2\n",
      "constantly\t1\n",
      "contains\t2\n",
      "context\t6\n",
      "context-relevant\t1\n",
      "continue\t2\n",
      "continued\t1\n",
      "continues\t1\n",
      "conventional\t1\n",
      "correlations\t1\n",
      "costs,\t2\n",
      "covid-19\t1\n",
      "created\t1\n",
      "creating\t1\n",
      "critical\t5\n",
      "crosses\t1\n",
      "curse\t1\n",
      "customer\t6\n",
      "customers\t1\n",
      "customers.\t3\n",
      "d&a\t2\n",
      "dark\t1\n",
      "data\t147\n",
      "data)\t1\n",
      "data).\t1\n",
      "data,\t8\n",
      "data-hungry\t6\n",
      "data-sharing\t1\n",
      "data.\t9\n",
      "data:\t1\n",
      "data;\t2\n",
      "databases\t2\n",
      "dataops\t2\n",
      "dataops,\t1\n",
      "datasets\t1\n",
      "data\t7\n",
      "deal\t2\n",
      "dealing\t5\n",
      "decade,\t1\n",
      "decades,\t1\n",
      "decision\t6\n",
      "decision-making.\t1\n",
      "deep\t4\n",
      "deeper\t1\n",
      "deliver\t1\n",
      "demand\t2\n",
      "demanding\t2\n",
      "demanding,\t1\n",
      "den\t2\n",
      "dependency\t2\n",
      "dependent\t1\n",
      "deployments.\t1\n",
      "deposit\t1\n",
      "derive\t1\n",
      "description:\t1\n",
      "designed\t1\n",
      "detect\t1\n",
      "detection\t1\n",
      "determining\t1\n",
      "developed\t1\n",
      "developing\t1\n",
      "development\t1\n",
      "devices\t6\n",
      "devices,\t1\n",
      "diagram\t1\n",
      "different\t2\n",
      "digital\t2\n",
      "digitized.\t1\n",
      "dimensionality)\t1\n",
      "disruptions\t1\n",
      "disruptions,\t2\n",
      "distributed\t3\n",
      "diverse\t4\n",
      "diversity\t4\n",
      "division\t1\n",
      "documents,\t1\n",
      "dominant\t1\n",
      "doubt\t2\n",
      "dramatically\t1\n",
      "drive\t2\n",
      "driven\t1\n",
      "driving\t3\n",
      "due\t2\n",
      "dynamic.\t1\n",
      "dynamics\t2\n",
      "each\t1\n",
      "easily\t2\n",
      "economic\t1\n",
      "edge\t4\n",
      "effectively,\t1\n",
      "efficiency\t1\n",
      "either\t1\n",
      "embodied\t1\n",
      "emerge.\t1\n",
      "emergence\t1\n",
      "emerging\t3\n",
      "emerging.\t2\n",
      "emotional\t1\n",
      "empowers\t1\n",
      "enable\t4\n",
      "enabler\t3\n",
      "enables\t4\n",
      "enabling\t3\n",
      "end\t1\n",
      "energy\t2\n",
      "enrich\t2\n",
      "enrichment\t1\n",
      "enterprise\t3\n",
      "enterprises\t7\n",
      "entry\t1\n",
      "entry.\t1\n",
      "environments\t2\n",
      "environments.\t1\n",
      "envisage\t1\n",
      "era,\t1\n",
      "eroding\t1\n",
      "especially\t4\n",
      "even\t6\n",
      "events\t1\n",
      "evidence\t1\n",
      "evolution\t2\n",
      "evolving\t1\n",
      "evolving.\t1\n",
      "exacerbated\t1\n",
      "example,\t3\n",
      "examples\t1\n",
      "exchanges\t1\n",
      "exciting\t1\n",
      "expanding\t1\n",
      "expected\t1\n",
      "expenses.\t1\n",
      "experience\t4\n",
      "expertise.\t1\n",
      "experts\t1\n",
      "explains\t1\n",
      "explanatory\t1\n",
      "explore\t1\n",
      "explosion\t1\n",
      "extend\t2\n",
      "external\t4\n",
      "extract,\t1\n",
      "extracting\t1\n",
      "familiar\t1\n",
      "far\t1\n",
      "farhan\t1\n",
      "fashion\t1\n",
      "faster\t1\n",
      "faster,\t1\n",
      "features\t1\n",
      "february\t1\n",
      "federated,\t2\n",
      "fell\t1\n",
      "few-shot\t2\n",
      "figure\t2\n",
      "files,\t1\n",
      "finance,\t1\n",
      "financial\t1\n",
      "find\t2\n",
      "finding\t3\n",
      "fitbit,\t1\n",
      "flow\t1\n",
      "flows\t1\n",
      "focus\t1\n",
      "focused\t1\n",
      "focuses\t1\n",
      "following:\t1\n",
      "for\t49\n",
      "forcing\t1\n",
      "fore\t1\n",
      "fore,\t1\n",
      "forecasting\t1\n",
      "foreseeable\t1\n",
      "form,\t1\n",
      "format.\t1\n",
      "formats\t2\n",
      "formats.\t2\n",
      "forms\t2\n",
      "forth\t1\n",
      "found\t1\n",
      "foundational\t1\n",
      "four\t2\n",
      "frameworks\t1\n",
      "fraud\t1\n",
      "from\t19\n",
      "full\t1\n",
      "further\t3\n",
      "future.\t1\n",
      "g00738992\t1\n",
      "gamut\t1\n",
      "gartner\t2\n",
      "gather\t1\n",
      "generated\t2\n",
      "generated,\t1\n",
      "generation\t2\n",
      "generation,\t2\n",
      "generators\t1\n",
      "get\t1\n",
      "google\t1\n",
      "google,\t1\n",
      "governance,\t2\n",
      "government\t1\n",
      "graph\t2\n",
      "graphs\t2\n",
      "greater\t4\n",
      "group\t2\n",
      "growing\t3\n",
      "growth\t2\n",
      "had\t1\n",
      "hadoop\t1\n",
      "hamer,\t2\n",
      "hand\t1\n",
      "handle\t3\n",
      "handled\t1\n",
      "hands\t1\n",
      "happen\t1\n",
      "happens\t1\n",
      "harder\t1\n",
      "hare\t1\n",
      "hare,\t1\n",
      "has\t3\n",
      "have\t5\n",
      "having\t2\n",
      "healthcare\t2\n",
      "healthcare,\t2\n",
      "heavily\t1\n",
      "help\t1\n",
      "helping\t2\n",
      "helps\t2\n",
      "here\t1\n",
      "here's\t1\n",
      "historical\t2\n",
      "how\t2\n",
      "https://www.techtarget.com/searchdatamanagement/feature/top-trends-in-big-data-for-2021-and-beyond\t1\n",
      "human\t1\n",
      "humans\t1\n",
      "hungry.\t1\n",
      "hybrid\t4\n",
      "hyperpersonalization\t1\n",
      "ibm\t1\n",
      "icons\t1\n",
      "id\t1\n",
      "identify\t1\n",
      "if\t1\n",
      "image,\t3\n",
      "images\t1\n",
      "images,\t1\n",
      "implement\t1\n",
      "implications:\t1\n",
      "improve\t5\n",
      "improved\t1\n",
      "improvement.\t1\n",
      "in\t69\n",
      "include\t9\n",
      "include,\t1\n",
      "includes\t2\n",
      "including\t2\n",
      "incorporating\t1\n",
      "increase\t1\n",
      "increased\t2\n",
      "increases\t2\n",
      "increasing\t2\n",
      "increasingly\t4\n",
      "increasingly,\t1\n",
      "indeed,\t1\n",
      "industries\t3\n",
      "industries.\t1\n",
      "industry\t1\n",
      "inexorable\t1\n",
      "information\t6\n",
      "information.\t2\n",
      "infrastructure\t3\n",
      "infrastructure,\t2\n",
      "infrastructure.\t2\n",
      "infrastructures\t1\n",
      "initiatives\t2\n",
      "initiatives,\t1\n",
      "initiatives:data\t1\n",
      "innovation\t3\n",
      "innovation)\t1\n",
      "innovation.\t1\n",
      "innovations\t3\n",
      "inquiries\t1\n",
      "insights\t1\n",
      "insights.\t2\n",
      "instead\t1\n",
      "instead,\t1\n",
      "insurance,\t1\n",
      "intelligence\t2\n",
      "intelligent\t1\n",
      "intelligent,\t1\n",
      "intentions\t1\n",
      "interactions\t1\n",
      "internal\t2\n",
      "international\t1\n",
      "internet\t1\n",
      "into\t4\n",
      "investing\t1\n",
      "iot\t2\n",
      "is\t28\n",
      "isn't\t1\n",
      "issues,\t1\n",
      "it\t9\n",
      "it's\t1\n",
      "it.\t1\n",
      "iterative\t1\n",
      "its\t3\n",
      "itself\t2\n",
      "jan\t1\n",
      "january\t1\n",
      "jim\t2\n",
      "jones\t1\n",
      "just\t2\n",
      "key\t1\n",
      "know\t1\n",
      "knowledge\t1\n",
      "known\t3\n",
      "labeled\t1\n",
      "lack\t2\n",
      "lake\t2\n",
      "lake.\t1\n",
      "lakes\t2\n",
      "language\t1\n",
      "language,\t1\n",
      "large\t6\n",
      "large,\t3\n",
      "largely\t1\n",
      "last\t1\n",
      "lax\t1\n",
      "leaders\t3\n",
      "learn\t1\n",
      "learning\t9\n",
      "learning,\t3\n",
      "learning.\t4\n",
      "leaves\t1\n",
      "led\t1\n",
      "left\t1\n",
      "legacy\t1\n",
      "less\t6\n",
      "let\t1\n",
      "levels\t1\n",
      "leverage\t1\n",
      "leveraging\t2\n",
      "liable\t1\n",
      "lifecycle\t2\n",
      "lifecycle,\t1\n",
      "lifecycle.\t1\n",
      "like\t1\n",
      "likewise,\t1\n",
      "limitations\t3\n",
      "limitations.\t1\n",
      "limited\t1\n",
      "limitless\t1\n",
      "link\t1\n",
      "links\t2\n",
      "load\t2\n",
      "logs,\t1\n",
      "look\t1\n",
      "low\t1\n",
      "lower\t2\n",
      "lowers\t1\n",
      "lydia\t1\n",
      "machine\t6\n",
      "main\t1\n",
      "maintain\t1\n",
      "major\t1\n",
      "make\t4\n",
      "makers\t1\n",
      "making\t4\n",
      "making,\t2\n",
      "manage\t2\n",
      "manage,\t1\n",
      "management\t2\n",
      "management,\t2\n",
      "managing\t2\n",
      "manner.\t1\n",
      "manufacturing\t1\n",
      "many\t5\n",
      "march\t1\n",
      "market\t3\n",
      "marketplaces\t2\n",
      "marketplaces.\t2\n",
      "markets.\t1\n",
      "massive\t1\n",
      "mean\t1\n",
      "meaning\t1\n",
      "means\t2\n",
      "media,\t2\n",
      "meet\t2\n",
      "methodology\t1\n",
      "microsoft,\t1\n",
      "min\t1\n",
      "ml\t1\n",
      "mobile\t1\n",
      "modeling\t1\n",
      "models\t1\n",
      "models,\t2\n",
      "models.\t1\n",
      "modern\t1\n",
      "monolithic\t1\n",
      "more\t30\n",
      "moreover,\t1\n",
      "mostly\t1\n",
      "motion,\t1\n",
      "move\t3\n",
      "moving\t3\n",
      "much\t3\n",
      "must\t1\n",
      "mutual\t1\n",
      "native\t1\n",
      "natural\t2\n",
      "need\t9\n",
      "needs\t5\n",
      "needs,\t1\n",
      "needs.\t2\n",
      "network,\t1\n",
      "networks.\t1\n",
      "new\t8\n",
      "next-generation\t1\n",
      "no\t3\n",
      "non-database\t1\n",
      "not\t2\n",
      "november\t1\n",
      "number\t2\n",
      "obsolete\t1\n",
      "of\t108\n",
      "offer\t2\n",
      "often\t2\n",
      "on\t16\n",
      "on-premises\t1\n",
      "one\t3\n",
      "one-size-fits-all\t2\n",
      "ones\t1\n",
      "ones.\t1\n",
      "open\t1\n",
      "operate.\t1\n",
      "operations\t1\n",
      "operations.\t1\n",
      "opportunities\t1\n",
      "optimal\t1\n",
      "optimization\t1\n",
      "optimize\t2\n",
      "optimized\t1\n",
      "optimizes\t1\n",
      "or\t15\n",
      "oracle\t1\n",
      "organization\t3\n",
      "organization.\t1\n",
      "organizational\t1\n",
      "organizations\t24\n",
      "organizations,\t2\n",
      "organizations.\t1\n",
      "organizations\t2\n",
      "other\t8\n",
      "outcomes.\t1\n",
      "over\t3\n",
      "overall\t1\n",
      "overlapping\t1\n",
      "overly\t2\n",
      "overview\t1\n",
      "own\t3\n",
      "pace\t1\n",
      "paired\t1\n",
      "pandemic\t1\n",
      "paper\t1\n",
      "particular,\t1\n",
      "partly\t1\n",
      "past\t4\n",
      "past,\t1\n",
      "patient\t2\n",
      "patterns\t2\n",
      "pay\t1\n",
      "people\t2\n",
      "perceived\t2\n",
      "performance\t1\n",
      "personal\t1\n",
      "personalization\t1\n",
      "personalized\t1\n",
      "perspectives\t1\n",
      "petabytes\t1\n",
      "physical\t1\n",
      "piecemeal\t1\n",
      "pieter\t2\n",
      "place\t1\n",
      "planning\t1\n",
      "plans\t1\n",
      "platforms\t1\n",
      "platforms,\t1\n",
      "plots.\t1\n",
      "potential\t1\n",
      "power\t4\n",
      "practice\t1\n",
      "practices\t3\n",
      "practices,\t1\n",
      "predictive\t3\n",
      "preparation\t1\n",
      "prevent\t1\n",
      "previous\t1\n",
      "previously\t1\n",
      "priorities\t1\n",
      "privacy\t3\n",
      "problems\t1\n",
      "process\t2\n",
      "process,\t1\n",
      "processes\t2\n",
      "processes,\t1\n",
      "processes.\t1\n",
      "processing\t13\n",
      "processing,\t3\n",
      "processing.\t3\n",
      "products\t1\n",
      "programs\t1\n",
      "progress\t1\n",
      "prohibitive.\t1\n",
      "promise\t1\n",
      "promoted\t1\n",
      "properly\t1\n",
      "protection\t1\n",
      "provide\t5\n",
      "providers\t3\n",
      "provides\t2\n",
      "providing\t1\n",
      "proving\t1\n",
      "public\t2\n",
      "public-sector\t1\n",
      "published\t2\n",
      "published:\t1\n",
      "putting\t1\n",
      "quality\t1\n",
      "quality,\t1\n",
      "quantities\t1\n",
      "query\t1\n",
      "questions\t1\n",
      "quickly.\t1\n",
      "ramp-up\t1\n",
      "range\t7\n",
      "rapid\t3\n",
      "rapidly\t2\n",
      "rate\t1\n",
      "rather\t5\n",
      "read\t1\n",
      "reading),\t1\n",
      "real\t3\n",
      "realizing\t1\n",
      "recent\t1\n",
      "recognition\t1\n",
      "recommendation\t1\n",
      "recommendations\t1\n",
      "recommended\t1\n",
      "reconsider,\t1\n",
      "reducing\t4\n",
      "reexamine\t1\n",
      "reflect\t1\n",
      "regulated\t1\n",
      "regulations\t1\n",
      "regulatory\t1\n",
      "regulatory-friendly\t1\n",
      "reinforcement,\t2\n",
      "relate\t1\n",
      "related\t3\n",
      "relevant\t1\n",
      "reliance\t1\n",
      "relying\t2\n",
      "remote\t1\n",
      "reporting\t1\n",
      "represented\t1\n",
      "require\t2\n",
      "required\t1\n",
      "requires\t1\n",
      "requiring\t2\n",
      "research\t1\n",
      "residing\t1\n",
      "resources\t1\n",
      "respondents\t1\n",
      "responses\t1\n",
      "responsibility\t2\n",
      "responsible\t2\n",
      "responsive\t1\n",
      "rest\t1\n",
      "restrictions\t1\n",
      "result\t1\n",
      "result,\t1\n",
      "resulting\t1\n",
      "results\t2\n",
      "retail,\t2\n",
      "revolutionary\t1\n",
      "richer\t2\n",
      "richer,\t2\n",
      "right\t2\n",
      "rise\t1\n",
      "robots,\t1\n",
      "robust\t3\n",
      "robustness,\t1\n",
      "ronald\t2\n",
      "said\t1\n",
      "scale.\t1\n",
      "schmelzer\t1\n",
      "schmelzer,\t1\n",
      "science\t1\n",
      "sea\t1\n",
      "sector,\t1\n",
      "secure\t2\n",
      "secured\t1\n",
      "security\t3\n",
      "see\t1\n",
      "seeing\t1\n",
      "seek\t1\n",
      "seeking.\t1\n",
      "self-supervised\t2\n",
      "semistructured\t2\n",
      "send\t1\n",
      "sensors,\t3\n",
      "sensory\t1\n",
      "sent\t1\n",
      "separate\t1\n",
      "servers.\t1\n",
      "service\t1\n",
      "services\t3\n",
      "services;\t1\n",
      "sets\t2\n",
      "shared\t1\n",
      "sharing\t2\n",
      "sharing,\t1\n",
      "shied\t1\n",
      "shift\t1\n",
      "shifting\t2\n",
      "shifts\t2\n",
      "short\t1\n",
      "should\t4\n",
      "should:\t1\n",
      "showing\t1\n",
      "shows\t1\n",
      "shubhangi\t1\n",
      "side\t1\n",
      "sign\t1\n",
      "significant\t2\n",
      "since\t1\n",
      "single\t1\n",
      "sitting\t1\n",
      "situation\t1\n",
      "situational\t4\n",
      "sizes\t2\n",
      "slow\t1\n",
      "small\t20\n",
      "small,\t1\n",
      "smart\t1\n",
      "smarter,\t1\n",
      "smartphones\t1\n",
      "smell\t2\n",
      "so\t1\n",
      "so-called\t1\n",
      "social\t2\n",
      "solutions\t1\n",
      "some\t2\n",
      "somewhat\t1\n",
      "source\t1\n",
      "sources\t6\n",
      "sources,\t5\n",
      "sources.\t5\n",
      "sourcing,\t1\n",
      "space\t2\n",
      "spark,\t1\n",
      "speed\t1\n",
      "speed.\t1\n",
      "spend\t1\n",
      "spending\t3\n",
      "spot\t1\n",
      "spur\t1\n",
      "spurring\t1\n",
      "staff.\t1\n",
      "standing\t2\n",
      "stay,\t1\n",
      "stays\t1\n",
      "stewardship\t2\n",
      "still\t3\n",
      "storage\t9\n",
      "storage,\t2\n",
      "store\t3\n",
      "stored\t1\n",
      "stores\t1\n",
      "storing\t2\n",
      "strategic\t1\n",
      "strategies\t1\n",
      "strategy\t4\n",
      "streaming\t1\n",
      "structured\t5\n",
      "structured,\t1\n",
      "such\t15\n",
      "sufficiently\t1\n",
      "summary\t1\n",
      "supervised\t1\n",
      "support\t4\n",
      "sure\t1\n",
      "survey\t1\n",
      "survey,\t1\n",
      "synergy\t2\n",
      "synthetic\t3\n",
      "system\t2\n",
      "systems\t7\n",
      "systems,\t2\n",
      "systems.\t2\n",
      "table\t1\n",
      "tabular,\t2\n",
      "tackle\t1\n",
      "tailored\t2\n",
      "taken\t1\n",
      "tangible\t1\n",
      "tasks\t1\n",
      "taxing\t1\n",
      "teams\t2\n",
      "technical\t2\n",
      "techniques\t14\n",
      "techniques,\t2\n",
      "technologies\t2\n",
      "technologies,\t2\n",
      "technologies.\t1\n",
      "technology\t5\n",
      "techtarget's\t1\n",
      "telemedicine\t1\n",
      "temperature\t1\n",
      "temperature,\t1\n",
      "text\t3\n",
      "text,\t2\n",
      "text.\t1\n",
      "than\t6\n",
      "that\t31\n",
      "the\t109\n",
      "their\t20\n",
      "them\t3\n",
      "themselves\t1\n",
      "themselves,\t1\n",
      "then\t1\n",
      "there\t2\n",
      "these\t7\n",
      "they\t3\n",
      "they're\t2\n",
      "things\t1\n",
      "think\t2\n",
      "thinking\t1\n",
      "third-party\t1\n",
      "this\t14\n",
      "this,\t2\n",
      "those\t3\n",
      "three\t1\n",
      "through\t9\n",
      "throughout\t1\n",
      "time\t2\n",
      "time-intensive\t1\n",
      "time-series\t2\n",
      "time.\t1\n",
      "to\t89\n",
      "to,\t1\n",
      "together,\t1\n",
      "toolbox\t2\n",
      "tools\t4\n",
      "top\t6\n",
      "toward\t1\n",
      "tracked\t1\n",
      "traditional\t2\n",
      "training\t1\n",
      "transactions\t1\n",
      "transfer\t2\n",
      "transform\t1\n",
      "transformation\t2\n",
      "translation:\t1\n",
      "transportation,\t1\n",
      "trend\t2\n",
      "trending:\t1\n",
      "trends\t9\n",
      "trends.\t1\n",
      "trust\t1\n",
      "trying\t1\n",
      "turn\t1\n",
      "turn,\t2\n",
      "turning\t1\n",
      "twins.\t2\n",
      "two\t1\n",
      "types\t2\n",
      "typically\t1\n",
      "understand\t1\n",
      "unprocessed\t1\n",
      "unstructured\t6\n",
      "unstructured,\t2\n",
      "unused\t1\n",
      "up\t1\n",
      "us\t1\n",
      "use\t8\n",
      "used\t2\n",
      "useful\t2\n",
      "user.\t1\n",
      "users\t2\n",
      "users,\t1\n",
      "users.\t1\n",
      "uses\t1\n",
      "using\t5\n",
      "usually\t1\n",
      "utilizing\t1\n",
      "v's\t2\n",
      "value\t3\n",
      "variety\t7\n",
      "variety,\t1\n",
      "varying\t1\n",
      "vashisth,\t1\n",
      "vast\t3\n",
      "vendor\t1\n",
      "veracity\t1\n",
      "via\t1\n",
      "vibration.\t2\n",
      "video\t2\n",
      "video,\t2\n",
      "videos,\t1\n",
      "view.\t2\n",
      "visibility\t2\n",
      "visualization\t2\n",
      "visualization.\t1\n",
      "visualized\t1\n",
      "voice\t3\n",
      "voice,\t2\n",
      "volume\t3\n",
      "volumes\t2\n",
      "voluminous\t1\n",
      "waiting\t1\n",
      "warehouse\t3\n",
      "was\t1\n",
      "watch\t1\n",
      "way\t1\n",
      "ways\t1\n",
      "we\t1\n",
      "wearables\t1\n",
      "web\t1\n",
      "websites\t1\n",
      "well\t3\n",
      "were\t1\n",
      "what\t3\n",
      "when\t2\n",
      "where\t1\n",
      "which\t3\n",
      "who\t1\n",
      "why\t1\n",
      "wide\t22\n",
      "wider\t1\n",
      "widespread\t1\n",
      "will\t5\n",
      "with\t22\n",
      "without\t4\n",
      "work\t1\n",
      "working\t2\n",
      "x\t5\n",
      "year\t1\n",
      "years\t1\n",
      "you\t1\n",
      "\t2\n",
      "big\t1\n",
      "small\t3\n",
      "wide\t3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_py\n",
    "\n",
    "hdfs dfs -cat ${work_dir}/${out_dir}/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. YARN jobs monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop also provided YARN Web UI for Yarn Resource manager. All the jobs (submitted, running or finished) can be traced in YARN Web UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YARN Web UI available at: https://jhas01.gsom.spbu.ru/user/vgarshin/proxy/8088/cluster\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'YARN Web UI available at:',\n",
    "    'https://jhas01.gsom.spbu.ru{}proxy/{}/cluster'.format(\n",
    "        os.environ['JUPYTERHUB_SERVICE_PREFIX'],\n",
    "        YARN_PORT\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. More MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Not only word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will count the number of reviews for each rating (1, 2, 3, 4, 5) in the [Kaggle Hotels Reviews dataset](https://www.kaggle.com/datasets/yash10kundu/hotel-reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ok price look hotel ok little run average cleanliness chose price seattle quite expensive, did n't room/bed reserved staff unhelpful, best westerns used nice hotels does n't fit mold, choose different hotel probably expensive time visit seattle,  \",2\r\n",
      "\"great choice wife chose best western quite bit research, looking near downtown free parking air conditioning pregnant hot, location good close space needle seattle offers free bus rides downtown area free wireless nice continental breakfast pretty impressive, room spacious bed fairly comfortable good pillows shower hot water good pressure, complaints desk aside john c. n't help, desk staff consisted young people mean n't social skills confidence answer good tourist questions, negotiate free parking told offered phone, nice cooperative parking issue, great value going place offers needs good big city price, note golden singha thai restaurant cedar short walk central downtown needle excellent food 8 dinner,  \",5\r\n",
      "\"good bed clean convenient just night happy stay hotel, clerks evening morning courteous helpful, bed comfortable, room clean comfortable, location good, easy walk trade convention center downtown.i left window open night air hear traffic noise not lot traffic noise, traffic noise lot like listening ocean washing beach.noise not issue hotel, n't bar music noisy people disturbance all.i definitely stay hotel seattle,  \",4\r\n",
      "\"deceptive staff deceptive desk staff claiming not park street 10pm 4am order try pay 15 parking fee, completely not true, parking free seattle street 6pm 8am.they deceptive signage outside building facing street saying no parking 10pm 4am wrong, public street falls seattle bylaws, locals know park nearby late-night club, save money park street plenty free space area stay travel lodge door offers free parking.as room organic fresh odor neutralizer stuff use nauseously overpowering, attempts eco-friendly lost blast heat window wide open order try aerate room, minor complaints, sucks room key big advertisement pizza company advertising overbearing signage room guess expected major chain,  \",2\r\n",
      "\"not impressed unfriendly staff checked asked higher floor 3rd floor highest lady desk told provide particular room second floor, used aaa rate guess restricted certain block rooms, fine got room place smelling badly food previous guest, windows wide open obviously rid smell, needless say freezing damp raining early june, checked downstairs complained, hesitated room, extremely unfriendly staff, breakfast ok heard guest comment staying years butthat hotel really gone downhill terms standards, hallways use bit uplift cleaned not recommend hotel all.the good thing use environmental friendly products bathrooms,  \",2\r\n",
      "\"best kept secret 3rd time staying charm, not 5-star ca n't beat, time stayed increased esteem, bw caters business crowd stayed leisure usually weekend, rate perfect, clean green request available drop-dead view space needle practically street, ask check-in make sure, mere walk belltown 5 short blocks choices restaurants wine spirits bars younger folks clubs live music pubs jazz clubs weather permits nice hoof pike, n't usual bw complimentary continental breakfast buffet b/c rates lower splurge breakfast room service, parking free lower locked parking cameras felt cool leaving shopping finds great shopping city discretely tucked away suv, check money not long best kept secret status longer, kudos staff management property,  \",5\r\n",
      "\"great location price view hotel great quick place sights.directly street space needle downtown tram science center downtown mall.amenities nice indoor pool kids secure underground parking garage great restaurant.rooms great size couples families.recommend hotel visiting like tostay downtown near sights,  \",4\r\n",
      "\"ok just looks nice modern outside, desk staff n't particularly friendly, corridors dark smelt steam cleaned carpet maybe good thing, hotel right space needle thrilling pulled mins walk away, unfortunately room rear view buildings parking lot, rooms space needle view.there no hotel laundry suprising larger hotel.hotel restaurant mistakes menu ordering new guy think numerous items left tray room service breakfast, phone promptly delivered,  \",2\r\n",
      "\"hotel theft ruined vacation hotel opened sept 17 2007 guests week, happy stumble scouting hotels apt stay booked gobcn- commented post unexpectedly cancelled arrival stuck place stay la merce festival barcelona biggest year, like trying hotel times square new year eve, vacancies hotel 1. new 2. not ready guests, new, hotel towels straight factory not washed pilling shower, not ready, entire week sept 21 28 slept proper blanket comforter n't arrived, got chilly night gave fleece throw supplied sister hotel ciutat barcelona hotel lime green colour scheme red black white spare cold, drawback double beds singles adjoined n't bothered hotels linens separately fish sheets order sleep couple fall lie middle, beds hard firm so-so quality foam pillows, best qualities hotel location close las ramblas hear street noise somewhat walkable attractions bari gottic, designed clean stylish hotel not clean 10 rooms construction things like cement drippings not scraped linoleum floor room etc., importantly room safe not securely bolted floor.i travel cash coming korea unfavorable exchange rate preferred not use canadian visa card unsure korean credit cards work overseas turns did husband did n't, carried moneybelt time life like heard barcelona petty theft problems witnessing mass humanity las ramblas wallet separated person blink eye, saw hotel safe stored money explicitly advised hotel security briefing form given guests check-in use hotel safe not leave valuable objects large cash room, no central safe reception meant use safe room, day stay discovered contents hotel room safe missing, n't open lock code n't working, receptionist came master code unlocked safe gone, 1,700 euros husband passport missing, incredibly shocked shaken, hours police report investigation staff fingerprinted room brushed fingerprints, later discovered guest fake belgian passport invalid credit card adjoining room broken terrace not room room room steal bag day prior, came room unbolted shelving safe bolted dragged shelf safe room dragged shelf safe, switched safes, used rolling carry-on bag stole room carry safe, talk ingenious crime.we suffered not theft aftermath primarily dealing hotel manager manages ciutat barcelona hotel not elusive refused meet person accessible telephone staff evasive compensated, end says expedited insurance process euros returned finally day departure barcelona going venice weekend waiting usually 2 month-long process grateful not compensate costs recovering passport husband nationality korean going consulate barcelona discover honorary does n't issue passports informed embassy madrid new temporary passport traveled madrid train day 9 hour+ return trip wasted day insurance cover stolen money said.after theft shaken no energy appetite tour sights wanted gaudi buildings park guell forever teenager planned months trip husband week year notoriously short korean vacations managed link weekends time vacation time particularly precious squeezed day, belongings felt secure hotel thereafter.they gave cheque compensating lost euros signing document spanish indemnified claims, did n't want sign document said n't cheque, needed cash point basically blackmailed signing document.to add insult injury checking informed hotel payable robbery thing hotel, balked offered 25 reduction, refused threatened police not paying hotel, unbelievable, think hotel extremely apologetic offer no charge stay night 3 days trip absolutely ruined, told gladly paid entirety hotel trip went just planned incident caused hotel negligence lack security flimsy locks not checking guests properly not informing guests room theft completely marred stay barcelona, said pay room theft occurred compensated costs recovering husband passport, reiterated insurance covered lost money n't true usually insurance cover entire contents no, guests stolen bag surely receive reimbursement insurance company, plan pursue hotel wonder vain real stickler did n't responsibility liability n't fault guest, unlucky., end plane catch paid nights robbery gladly left hotel.incredibly attests petty crime problem barcelona absolutely beautiful city hotel guest room got bag stolen restaurant chair eating dinner, shaken accompanied police station file report, returned hotel n't fixed phone connection needed contact family boyfriend germany appeared unsympathetic, problems wi-fi connection, theft staff fine save manager professional apologetic theft, manager offered weakest apology night occured hard-line meant lot met face face, minimum heartfelt apology opinion.in mossos d'esquadra barcelona police young undercover vigilant excellent thorough impressive, got story theft bit bit uncovered things not hotel gave news, opposed random pickpocketing problem barcelona working hard resolve n't want lose tourists affects barcelonins treated crime quite seriously appeared inside job.all incredibly sad disappointing frustrating experience, hotel reacted affected things great deal stand proper felt free actually tour sights incident knowing working hard recover loss instead feeling insecure victimized.this hotel clearly not ready guests opened security properly established, cctv cameras helpful not entrance affixed terrace building, night area hopping wee hours c. tallers hotel located brightly lit deter muggings 4 star hotels corner.one receptionists said hotel overpriced 3 star hotel paying location newness minimalist style hotel, said walks 40 euros pocket.there wealth good hotel rooms apt stay opportunities barcelona city fabulous encourage seek,  \",1\r\n",
      "\"people talking, ca n't believe excellent ratings hotel, just n't, yes patricia extremely helpful fluent languages goes way make welcome, said place bit dump, inexpensive hotel expensive city place bit dated institutional odor charm funeral home, walking steps hotel girlfriend step condom yes condom steps, lots guys hanging desk hallway, girlfriend swears house prostitution, patricia did arrange taxi following morning stayed night did wrong information cost fare, rooms clean large bathroom small, passable night glad leave following morning not recommend extended stay unless tight budget n't care look feel place,  \",2\r\n"
     ]
    }
   ],
   "source": [
    "!tail ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1421\n",
      "2\t1793\n",
      "3\t2184\n",
      "4\t6039\n",
      "5\t9054\n",
      "Rating\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# test our scripts\n",
    "\n",
    "cat ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv | \\\n",
    "    python ./utils/mapper_rr.py | \\\n",
    "    sort -t 1 | \\\n",
    "    python ./utils/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir {WORK_DIR}/input_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now put local file to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv {WORK_DIR}/input_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now put MORE local file to HDFS (cause we can do it!)\n",
    "# we can add many files and that is how Hadoop works\n",
    "# because we can have thousands of CSV files all across many servers\n",
    "\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv {WORK_DIR}/input_csv/Hotel_Reviews_more.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan hadoopusers   14966021 2023-05-02 15:32 /jovyan/input_csv/Hotel_Reviews.csv\r\n",
      "-rw-r--r--   1 jovyan hadoopusers   14966021 2023-05-02 15:32 /jovyan/input_csv/Hotel_Reviews_more.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/input_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_csv\n",
    "\n",
    "# delete directory if exists\n",
    "hdfs dfs -rm -r ${work_dir}${out_dir}\n",
    "\n",
    "yarn jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.2.jar \\\n",
    "    -input ${work_dir}/input_csv/*.csv -output ${work_dir}${out_dir} \\\n",
    "    -file ./utils/mapper_rr.py -file ./utils/reducer.py \\\n",
    "    -mapper \"python3 mapper_rr.py\" -reducer \"python3 reducer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_csv\n",
    "\n",
    "hdfs dfs -ls ${work_dir}/${out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/output_csv\n",
    "\n",
    "hdfs dfs -cat ${work_dir}/${out_dir}/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Meet MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module [mrjob](https://mrjob.readthedocs.io/en/latest/) helps to write MapReduce jobs in Python 2.7/3.4+ and run them on many platforms:\n",
    "- Write multi-step MapReduce jobs in pure Python\n",
    "- Test on your local machine\n",
    "- Run on a Hadoop cluster\n",
    "- Run in the cloud using Amazon Elastic MapReduce (EMR)\n",
    "- Run in the cloud using Google Cloud Dataproc (Dataproc)\n",
    "- Easily run Spark jobs on EMR or your own Hadoop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to write Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./utils/mrjob_ratings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# test mrjob script locally \n",
    "# only Python works with no YARN, Hadoop, HDFS etc.\n",
    "\n",
    "python ./utils/mrjob_ratings.py \\\n",
    "    ~/__DATA/IBDT_Spring_2023/topic_3/Hotel_Reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan/input_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put local file to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/test_hdfs.txt /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# now let's run mrjob script within Hadoop \n",
    "# NOTE: python3 is used, it os a feature of mrjob\n",
    "\n",
    "python3 ./utils/mrjob_ratings.py \\\n",
    "    --python-bin /opt/conda/bin/python3 \\\n",
    "    -r hadoop hdfs:///jovyan/input_csv/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traces of mrjob in the HDFS (logs, outputs etc.)\n",
    "!hdfs dfs -ls /user/jovyan/tmp/mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. MRJob for crypto currencies analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cryptocurrency Price & Market Data dataset](https://www.kaggle.com/datasets/thedevastator/cryptocurrency-price-market-data) provides the insights into the cryptocurrency markets. It collects important data points such as:\n",
    "- name of the cryptocurrency\n",
    "- symbol\n",
    "- price\n",
    "- hourly and daily change trends\n",
    "- 24 hour volume traded\n",
    "- market capitalization\n",
    "\n",
    "Our goal will be to find top-10 `24 hour volume traded` crypto currencies with the help of `mrjob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ~/__DATA/IBDT_Spring_2023/topic_3/coin_gecko_2022-03-17.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir {WORK_DIR}/input_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data to HDFS\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2023/topic_3/coin_gecko_2022-03-17.csv {WORK_DIR}/input_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/input_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# test mrjob script locally \n",
    "# only Python works with no YARN, Hadoop, HDFS etc.\n",
    "\n",
    "python ./utils/mrjob_crypto.py \\\n",
    "    ~/__DATA/IBDT_Spring_2023/topic_3/coin_gecko_2022-03-17.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# now let's run mrjob script within Hadoop \n",
    "# NOTE: python3 is used, it is a feature of mrjob\n",
    "\n",
    "python3 ./utils/mrjob_crypto.py \\\n",
    "    --python-bin /opt/conda/bin/python3 \\\n",
    "    -r hadoop hdfs:///jovyan/input_crypto/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [Video Game Sales dataset](https://www.kaggle.com/datasets/gregorut/videogamesales) that contains a list of video games with sales greater than 100,000 copies\n",
    "\n",
    "Fields of the dataset include:\n",
    "- Rank - Ranking of overall sales\n",
    "- Name - The games name\n",
    "- Platform - Platform of the games release (i.e. PC,PS4, etc.)\n",
    "- Year - Year of the game's release\n",
    "- Genre - Genre of the game\n",
    "- Publisher - Publisher of the game\n",
    "- NA_Sales - Sales in North America (in millions)\n",
    "- EU_Sales - Sales in Europe (in millions)\n",
    "- JP_Sales - Sales in Japan (in millions)\n",
    "- Other_Sales - Sales in the rest of the world (in millions)\n",
    "- Global_Sales - Total worldwide sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the Video Game Sales dataset loaded\n",
    "!ls ~/__DATA/IBDT_Spring_2023/topic_3/vgsales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your home assignment for this part is:\n",
    "1. Take the `vgsales.csv` and load it to HDFS\n",
    "2. Count the number of video games by the platform (field `Platform` in the file)\n",
    "3. Find top-5 video games by sales in Japan (field `JP_Sales`)\n",
    "\n",
    "Please use `mrjob` library count for the tasks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
